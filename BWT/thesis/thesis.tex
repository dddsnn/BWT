\documentclass[a4paper]{scrreprt}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xparse}
\usepackage{array}

\NewDocumentCommand{\rot}{O{45} O{1em} m}{\makebox[#2][l]{\rotatebox{#1}{#3}}}
\algnewcommand\True{\textbf{true}\space}
\algnewcommand\False{\textbf{false}\space}
\newcolumntype{t}{>{\fontfamily{\ttdefault}\selectfont}c}

\title{Burrows-Wheeler compression with modified sort orders and exceptions to
the MTF phase, and their impact on the compression rate}
\date{September 29, 2014}
\author{Marc Lehmann}

\begin{document}

\maketitle

\begin{abstract}
asdf
%TODO
\end{abstract}
%TODO eidestattl erkl.
\chapter{Intro}

%-def compression (decompression necessary)
%-explain bw-mtf-entropy, mtf can be skipped
%-explain simple version of bw used here, no rle
%-definitions: context block, symbol (byte in this thesis), natural sort order,
%	transition, order, sort something according to an order, sort by something
% (key))

The Burrows-Wheeler compression algorithm is a highly effective context-based
compression method centered around the Burrows-Wheeler Transform (BWT) first
described in 1994\cite{burrowswheeler1994bwt}.
The basic compression is done in three stages:
\begin{enumerate}
  \item Burrows-Wheeler Transform of the input, to produce a permutation that
  groups ``similar'' symbols together.
  \item Move-To-Front (MTF) coding of the first stage's output, to produce a
  sequence of natural numbers, with lower values occurring much more frequently
  than higher ones.
  \item Compression of the MTF code with an entropy coder, such as Huffman
  coding.
\end{enumerate}
There are variations of the algorithm, most notably run-length encoding of the
output of the second stage to make use of long runs of zeros that tend to occur.
This modification was already proposed by Burrows and Wheeler in their original
paper and can lead to considerable compression gains.

However, I will focus on only the three core stages, as these (in particular
the first two) are the ones affected by the modifications I
examine\footnote{I'm also lazy.}

I will briefly describe the stages as I use them in my implementation.

\section{The Burrows-Wheeler Transtorm}

The Burrows-Wheeler Transform transforms an input string into a permutation of
that string that is more suited for compression. It does not itself perform any
compression, in fact, in addition to the permuted string a start index is
necessary to reverse the transformation, so it actually slightly inflates the
input.

To perform the transformation, all cyclic shifts of the input are created and
placed one below the other so they form a table that has as many rows and
columns as the input string is long. The table is then sorted; the output of the
transform is the last column of the sorted table.

For example, the string \texttt{mississippi} yields the following table:

\begin{table}
\begin{tabular}{|r||tttttttttt|t|}
\hline
0 & i & m & i & s & s & i & s & s & i & p & p \\
1 & i & p & p & i & m & i & s & s & i & s & s \\
2 & i & s & s & i & p & p & i & m & i & s & s \\
3 & i & s & s & i & s & s & i & p & p & i & m \\
4 & m & i & s & s & i & s & s & i & p & p & i \\
5 & p & i & m & i & s & s & i & s & s & i & p \\
6 & p & p & i & m & i & s & s & i & s & s & i \\
7 & s & i & p & p & i & m & i & s & s & i & s \\
8 & s & i & s & s & i & p & p & i & m & i & s \\
9 & s & s & i & p & p & i & m & i & s & s & i \\
10 & s & s & i & s & s & i & p & p & i & m & i \\
\hline
\end{tabular}
%TODO
\caption{example table}
\end{table}

The output of the transformation in this case is \texttt{pssmipissii}. In
addition, the index of the row where the first character of the input string
appears in the last column is saved (3 in this case).

To perform the inverse transform, it is important to note that every index of
the input string appears exactly once in each column of the table, and that
therefore every column is a permutation of the input.

Given only the last column of the table, the first column can be reconstructed
by simply sorting the last. Since the rows wrap around from the last column to
the first, the \(i\)-th character in the last column is followed by the \(i\)-th
character in the first column, so it's easy to find the next character for any
character in the last column.

The trickier part is, given an index in the first column, to find the correct
index in the last column at which to continue decoding. If the character \(c\)
at the given index occurs for the \(i\)-th time in the first column, it
corresponds to the index in the last column at which \(c\) occurs for the
\(i\)-th time. This is due to the fact that the table is sorted: All the \(c\)'s
are grouped together in the first column and are sorted based on the strings
\(s_i\) succeeding them, with \(s_i\) denoting the string preceded by the
\(i\)-th occurrence of \(c\) in the first column. Each of the \(s_i\) is also
the beginning of a row in the table, and the relative order between the \(s_i\)
is the same whether they're preceded by \(c\), or they're the beginning of a
row, i.e. the row beginning with \(s_i\) appears before the row beginning with
\(s_j\) iff \(i<j\).

%TODO algorithm to match first col index to last col index, see multi order
% chapter

\section{Move-To-Front Coding}

Move-To-Front coding transforms an input string into a sequence of integers
denoting the index of the encoded character in the coders alphabet. When
starting encoding, the alphabet is initialized to a known value (usually
something trivial, e.g. [a, b, c, \ldots, x, y, z] if only lowercase letters
have to be encoded). The alphabet is updated after each character is encoded, so
that the character is moved to the front of the alphabet (and thus its index
becomes 0), with all characters before its previous position being shifted to
the back.

For example, the MTF code of the string \texttt{aabccca} with the starting
alphabet [a, b, c] is [0, 0, 1, 2, 0, 0, 2].

MTF coding is easily reversible, as the starting alphabet is known, and the
alphabet can be updated with every decoded character, just as during encoding.

Starting alphabet in my implementation [0x00, 0x01, \ldots, 0xff] because byte
values are symbols.

\section{Entropy Coding}

Entropy coding encodes input characters into variable length codes, based on
statistical properties of the input. Characters that occur more frequently are
assigned shorter codes than those that occur only rarely.

%Huffman coding, optimal, arithmetic coding, my implementation

\section{Definitions}

\chapter{Simple Sorting}

%-overwork in context transitions
%-metrics to rank transitions, tsp to find a tour
%-problem finding atsp heuristic, only one i found only supports ints, give 
%	error bounds for the scaling
%-chapins metrics work on the bw code
%-badness works on the mtf of the bw, with modifications attempts to give the
%	number of bits a transition costs:
%	-make a list of the best possible alphabet the left side of a transition
%		can leave for the right side
%	-compare those with the actual codes you get when you mtf encode the left
%		and right side together
%	-the sum of all the differences is the badness
%	-variants:
%		-weighting: divide the badness by the number of new symbols on the right
%			side, so transitions to a larger context don't get a massive
%			penalty
%		-new penalty: new symbols in the right side that didn't appear in the
%			left side need to receive a penalty instead of assuming the minimum
%			possible (mean mtf code for mtf codes of at least the min possible,
%			symbol specific)
%			prediction for the block that ends up as the first will be way off
%		-entropy code len: don't take the plain difference but the difference
%			between the number of bits the entropy coder will use. this can
%			give you the exact number of bits this transition costs
%			approximation with const values for zeros: zeros will all get the
%			same (longest possible) code length, other rare symbols may be
%			affected
%			approximation with curve fitting: zeros will get about the same
%			code lengths as the surrounding symbols, but all of the rarer
%			symbols (>10?) will have (significantly) longer codes as there are
%			more symbols to encode. higher value mtf codes are usually sparse,
%			but can't be sure how many zeros will remain after reordering
%			curve fitting sparse: assume there will be the same amount of
%			zeros after reordering. give zeros the same length as their
%			neighbors, even though there can't be a huffman code with those
%			code lengths and that many symbols 
%	-describe things needed for an ideal badness metric (new penalty,entropy
%		len, weighting?), then give approximations
%TODO	-another point of non-optimality: which transition to choose as a starting
%		point? need to break up the baddest transition
%	-need need a new penalty dict that is specific for the symbol being encoded
%		i.e. what is the average code for e.g. 'a' given that the minimum
%		possible is n
%	-new penalty predictor that gives the average code for a specific symbol
% 		that is new in the block, with a given mimimum
%	-much higher compression benefit when sorting all columns instead of just
%		the first one -> a lot of the compression benefit comes from the higher
% 		order transitions
%	-after comparing predictions with actual values, do another pass of
%		calculating transitions, but pass a compensation summand that will be
%		added to every prediction, so that the average is 0

The reason Burrows-Wheeler based compression algorithms are so effective is that
the Burrows-Wheeler Transform (BWT) creates sequences of symbols that appear in
the same context. If the input file is suitable for context-based
compression, these sequences will consist of only a few distinct symbols with
lots of repetition, leading to many small numbers in the MTF code.

%TODO describe overwork better

But any time the context changes, in the Burrows-Wheeler code a new sequence
begins that is in general not related to the previous one. This incurs some
overwork\cite{bitner1979heuristics} for the MTF coder as it has to adapt to the
new context, resulting in larger codes for symbols that were not requested in
the previous sequence.

%TODO why doesn't this affect decompressibility
%TODO overhead needed to store reordering

The idea behind using different sort orders is to order the contexts in a way
that reduces the overwork caused by a transition from one context to the next,
resulting in lower MTF codes and higher compression.

In order to find a suitable reordering, you need to do the BWT on the input data
to get the context blocks, rank all possible transitions between blocks with a
cost metric and find a sort order based on the costs. Finding the ordering with
given costs is an instance of the traveling salesman problem, where the nodes
are the context blocks and the distances between them are the cost of the
transition.

\section{Related Work}

chapin original \cite{chapin1998sort,chapin2001diss}
merge with next section unless there's more

\section{Chapin's Metrics}

%TODO aeiou

Chapin uses four different cost metrics, three of which I have reimplemented for
comparison with my own metric.\footnote{The fourth uses the Kullback-Leibler
distance, but is unclear about how to deal with zeros in the PMF.} All of them
analyze the histograms of symbol appearances for every context block and try to
give a measure of how similar they are. That means: For each context block
(generated by BW encoding the input with the natural sort order), make a
histogram of the number of symbol appearances in that block, i.e.
a mapping from each possible symbol to the number of times it appears in that
block.

In Chapin's first metric, two histograms are compared by taking the
logarithm\footnote{Chapin doesn't specify to which base. In my implementation,
I use the natural logarithm.} of all of the symbol counts, calculating, for each
symbol, the differences between the logarithms in both histograms and summing up
the squares of all of the differences. The result is used as a measure of
similarity between the two contexts.

For the second metric, the symbol counts from the histograms are written to a
list in decreasing order. The cost of a transition is the number of
inversions\cite{sleator1985amortized} between the two corresponding lists.
%TODO shitty cite

The third metric is just the logarithm of the second metric.\footnote{Again, no
base was specified, I use the natural logarithm.}

-not comparable with my data because of variations in tsp and the actual
compression algo

\section{The Badness Metric}

My own metric attempts to give a value denoting how bad any given transition is
and is hence called the badness metric.\footnote{Only after I had given it that
name and used it all over my code did it come to my mind that assigning a
badness value is really the purpose of any such metric.} Other than Chapin's
metrics, it doesn't operate on the BW code, but on the MTF code it generates
and so, in a sense, is ``closer to the compression''.


\subsection{Ugly LKH}

The badness values generated are not necessarily symmetric, i.e. the badness of
the transition from context x to y is not necessarily the badness of y to x.
This has the unfortunate consequence that I now need a heuristic solver for the
asymmetrical traveling salesman problem, which is much harder to find than a
heuristic for the ``ordinary'' symmetrical TSP. I finally found
LKH\cite{helsgaun2000lkh}\footnote{http://www.akira.ruc.dk/~keld/research/LKH/},
which can approximate solutions to the asymmetrical TSP. Unfortunately, it can
not handle floating point input, so I have to do some ugly scaling.

The transition data is exported as a .atsp file in full matrix format according
to the TSPLIB specification\cite{reinelt1991tsplib}. LKH doesn't allow floating
point numbers in the matrix, so the original values have to be converted into
integers while preserving the ratios between all of the values as best as
possible. Since just rounding to the nearest integer would incur large relative
errors for values close to zero, they are first scaled up. If all values are
multiplied with the same factor, the solution to the TSP remains the same. This
factor is the greatest number such that the absolute greatest original value,
when multiplied with the factor, is less than or equal to a preset maximum. If
this maximum is too large, LKH will fail an assertion, probably because the
variable overflows. I have chosen \(10^7\), as this seems to be small
enough to not cause problems.
%TODO still 2*10^7?

To get a picture of the accuracy of this transformation, for every pair of
values the ratio of the original (floating-point) values and the ratio of their
correspoding integers is computed and the relative error calculated. The maximum
relative error for the examples is rarely greater than \(10^-4\).
%TODO update in case this changes
%TODO inverse of scaling factor
LKH is run with default parameters, with the exception of the number of runs,
which is 100 rather than the default 10.

\subsection{Basic Operation}

To explain the badness metric, I need to introduce the partial MTF code that
will be used on appropriate parts of the BW code. This is not absolutely
necessary to get the actual badness value, but useful to understand why the
metric does what it does.

The partial MTF differs from the regular MTF code only in that you start with an
empty alphabet and encode an escape code (I will use -1) anytime a symbol
appears that isn't already in the alphabet.
E.g., the ASCII encoded string ``aabcab'' encoded (byte-wise) with regular MTF
would yield \([97, 0, 98, 99, 2, 2]\), encoded with partial MTF it would be
\([-1, 0, -1, -1, 2, 2]\).

%TODO property non-escape codes are the same

Of course this means that it can't be decoded anymore, but we only need it to
illustrate the operation of the metric.

To compute the badness value of a transition, the metric needs the context
blocks for both sides of the transition, i.e. the BW code belonging to the
symbol that's being transitioned from or to, respectively.

It then produces the partial MTF code of the destination block (the right
side), and of the concatenation of the source and destination block (combined
partial MTF).
So it  basically pretends that the two blocks were sorted one after the other,
and then attempts to give an indication of how bad this would be for the
compression.

Looking at the partial MTF code of the destination side of the transition, the
metric assumes that all recurring codes (i.e. those that aren't escape codes)
are not interesting for the purposes of ranking transitions, as they would be
the same no matter what comes before or after.\footnote{This assumtion is not
actually correct, as I will explain in the next section.}

What is of interest, are the escape codes on the right side of the transition,
because this is where what has previously been encoded can influence the MTF
codes in the final result. When the input file is finally encoded with the
(full) MTF, each of the escape symbols will be replaced by an actual code. How
high those codes are, determines the compression lost by the transition between
contexts.

The metric compiles the ideal MTF alphabet the left side can ``leave'' for the
right side.

E.g., the first code in the partial MTF of the right side is necessarily an
escape code. It would be ideal, if the last symbol of the left side were the
first symbol of the right side. That would mean that it is at the front of the
alphabet when the transition occurrs, and so that first escape code would, in
the full MTF be encoded as a 0 (and smaller codes are better). The ideal full
MTF code for the next escape code is 1 (since the code 0 in the alphabet has
already been taken by whatever symbol came before it) etc.
%TODO example? pseudo code?

The metric can then compare the ideal codes with the actual codes in the
combined partial MTF at the positions where the escape symbols are in the
partial MTF of just the right side. The differences between actual code and
ideal code are summed up to form the badness of the transition.

What can also happen is that the part of the combined partial MTF belonging to
the right side still contains escape codes. This means that the symbols encoded
by them do not appear in the left side of the transition at all. In this case,
the metric assumes the best possible code for that symbol, which is equal to the
number of escape symbols in the combined partial MTF up to that point (i.e. the
metric assumes that whatever context block ends up preceding the left side
of this transition will leave an ideal alphabet).

\subsection{Ideality, under some circumstances}

The metric assumes that the partial MTF codes of the right side of the
transition that are not escape codes don't change no matter the order that is
finally selected, and thus aren't interesting. This assumption is incorrect
since, when a different reordering is selected, the BW code of the context
blocks is also reordered, and so the generated MTF code will (probably) be
different. For Chapin's metrics, this doesn't matter because they only care
which symbol occurrs how often, not in which order.

So we'll consider a slightly different problem than finding an ideal global
reordering: instead of looking for a sort order by which the entire BW table
will be sorted, we look for an order by which only the first column of the BW
table will be sorted, all other columns will use the natural order (which was
used to generate the context blocks that are fed to the metric).

%TODO still reversible, see next chapter

This is equivalent to finding a reordering of the fixed context blocks the
metric deals with. This means that recurring symbols in the partial MTF never
change, no matter what order is chosen for the first column, and so the
assumption the metric makes is true.

Let's further assume that we have two oracles that know which reordering will
finally be selected. When the metric encounters an escape symbol in the right
side of the combined partial MTF (i.e. a symbol that appears in the right side
doesn't appear in the left side), it has to assume the best possible code for
lack of information. The first oracle can provide the actual codes that will be
at those positions in the final computed order.

The second oracle can provide the codeword lengths in bits of the entropy coder
for every MTF code appearing in the final reordered and MTF encoded file.

If the badness metric is modified to use the predictions of the MTF code oracle
and sums up the differences between the entropy codeword lengths of the actual
and ideal MTF codes instead of the differences between the codes themselves, it
is the ideal metric for generating a reordering for only the first column.

Since only the first column gets reordered, recurring symbols within a
context block are fixed and have no influence, good or bad, on the compression
rate. The only thing that can make a difference are the MTF codes of symbols
occurring for the first time in the block. The badness metric records the
difference between the number of bits that are used if the left side of the
transition leaves an ideal alphabet for the right side, and the number of bits
that are actually used.

That way, if the transition (a,b) has a badness of n, and the transition (a,c)
has a badness of n+m, all other transition's badness values being equal, if the
transition (a,c) is used, the result will be m bits larger than if the
transition (a,b) was used.

If the badness values of all possible transitions are provided as input to an
exact TSP solver, it will generate the best possible reordering for only the
first column of the BW table.

\subsection{Variants}

There are three variants of the badness metric, two of which are attempting to
approximate the predictions of the oracles from the previous section.

\subsubsection{Weighting by number of symbols}

The computed badness value is divided by the number of distinct symbols in the
right side of the transition (i.e. the number of escape codes in the partial
MTF). This is supposed to counteract the effect where transitions whose right
side has many differenct symbols get a much worse rating than if they have less,
because every escape code leads to some badness being added to the total value.
The resulting value can be characterized as badness per distinct symbol.

\subsubsection{Predicting code lengths of the entropy coder}

%TODO: arithmetic coding? otherwise just call it Huffman coding
This modification tries to approximate the behavior of the oracle predicting the
code lengths of the entropy coder.

When adding to the badness value, instead of adding the difference between the
actual and the ideal MTF codes, the difference between a prediction of the
entropy code lengths of the actual and ideal MTF codes is added.

Getting pretty good predictions for a static entropy coder is relatively easy:
since different reorderings only has a little effect on the produced MTF code,
the entropy code lengths for any reordering should be within a small margin of
error of each other. In the case of static Huffman coding, many of the codes for
low MTF codes may not change at all.

So in order to get the predictions, the input file is simply BW- and MTF encoded
with the natural sort order and the lengths of the codewords are recorded that
the entropy coder would use for each MTF code.

There is a problem with this simple method when not all possible MTF codes
appear in the code that is used to make the predictions. This is usually the
case with e.g. ASCII text files, which tend to use less than 100 distinct
symbols\footnote{book1 from the calgary corpus uses 82 different characters:
upper- and lowercase alphabet, 10 digits and some punctuation and control
characters.}, so MTF codes above that number only occur once when that symbol is
fetched to the front of the MTF alphabet, and may not occur at all.
When a different reordering is chosen, a different MTF alphabet will probably be
in place at the time such a symbol is requested, and the resulting MTF code may
be shifted a little. That means that an MTF code for which a prediction exists
may not be requested at all, but a code that is requested has no prediction.

To solve this, I have developed two strategies, ``complete'' and ``sparse''
predictions.
The ``complete'' method modifies the symbol frequencies of the MTF code that are
used as weights by the entropy coder. Every MTF code that doesn't appear but is
smaller than the maximum code that does gets the weight \(\frac{1}{256}\). In
the case of Huffman coding this means that all these codes get a longer or
equally long entropy code than the codes that actually appear.

All other MTF codes that don't appear, but are greater than the maximum code
that does get weight \(\frac{1}{256^2}\), so their entropy codes are guaranteed
to be longer or equal to the ones with weight \(\frac{1}{256}\). The reasoning
behind this is that these codes are unlikely to appear in any reordering, e.g.
when encoding an ASCII text file, no codes above 127 will appear.

The ``sparse'' method assumes that no matter the reordering, there will always
be (high) MTF codes that don't appear (the histogram of MTF codes will be
sparse for high values).
It lets the entropy coder compute the codeword lengths with unaltered weights,
but inserts entropy code lengths for the missing MTF codes manually afterwards.
Each missing MTF code is assigned the same code length as the next smaller MTF
code that does appear. The reasoning is that, when MTF codes appear for one
reordering that didn't for another, they're just shifted around from codes that
don't appear anymore. Of course this means that there isn't actually an entropy
code with the code lengths that were predicted (since the predictor has
manually introduced collisions).

\subsubsection{Predicting MTF codes for new symbols}

This modification aims at approximating the oracle that can predict MTF codes of
escape codes in the right side of the combined partial MTF of a transition. When
the metric encounters an escape code, instead of assuming that whatever
context came before left an ideal alphabet, it can use the prediction, which is
hopefully more accurate.

Making these predictions is more complicated than those for the entropy code
lengths. I have again developed two strategies, ``generic'' and ``specific'',
which assume that the distribution of MTF codes will be similar no matter the
order.

The generic predictor does a BW encode on the input file with the natural order,
then encodes this with MTF. It then records, for every possible MTF code, the
average value of all MTF codes that are greater or equal to it.

The specific predictor also takes into account the underlying BW code. So it
records, for every possible MTF code and every possible symbol, the average
value of the MTF codes that are greater or equal to it and that encode the
underlying symbol.

The averaging function for both predictors can be the arithmetic mean or the
median.

\subsection{Evaluating the performance of the predictors}

Besides just trying out how much of an impact the different predictors have on
the final comression rate, we can also evaluate their performance by simply
comparing the values they predicted with the values that actually appear in the
reordered MTF code.

\subsubsection{MTF predictor}

For this purpose, every prediction of the MTF code predictor is logged while the
metric is computed. It is recorded in which transition the prediction happened,
which symbol of the underlying BW code is encoded by the MTF code that has to be
predicted, and the predicted value itself.

Once the TSP heuristic has computed the reordering according to the metric,
only the predictions of the transitions that occur in the reordering can be
evaluated. The predictions for the first context block in the reordering also
can't be evaluated, since predictions only happen in the right side of a
transition, and the first block is not the right side of any transition that
appears in the reordering.

%TODO only describe the ones used, put the rest in appendix

When we have all the pairs (actual value, predicted value), we can calculate
indicators that can hint at the quality of the predictions:
\begin{itemize}
  \item The mean difference, i.e. the mean over all the (actual - predicted)
  pairs
  \item The mean deviation, i.e. the same as the mean difference, only with the
  absolute of the difference
  \item The (kind of) standard deviation, i.e. the square root of the mean of
  the squared differences between actual and predicted value.\footnote{This is
  not a real standard deviation, because we don't measure it with the distances
  from a single expected value, since each prediction gets its own actual
  value, the distance to which matters for the deviation. It's also not clear
  that the distances follow a normal distribution, and the distribution is much
  more spread out when the predicted value is less than the actual value than
  when it is greater.}
\end{itemize}

The mean difference can tell us how well the predictions are on average and
should be close to zero. If it is significantly greater or less, it means that
the predictor is too optimistic or pessimistic, respectively.

The deviations can tell us how far the predictions are spread around the correct
value, and should be as low as possible.

We can also calculate these indicators with just a subset of the
(predicted, actual) pairs:
\begin{itemize}
  \item We can exclude pairs where the actual value is
  greater than the number of distinct symbols in the input file. They only occur
  when a symbol that's never been encoded before needs to be fetched to the
  front of the MTF alphabet. They occur at most as many times as there are
  distinct symbols in the input, but they skew the averages significantly.
  \item We can only take pairs where the prediction is either greater than or
  equal, or less than the actual value. With this we can learn how far spread
  out the distribution is to either side.
\end{itemize}

Finally, we can calculate all these indicators while not using the difference
between the codes themselves, but between their entropy code lengths. This can
give us an indication of how big the error is in bits a prediction is likely to
cause.

\subsubsection{Entropy length predictor}

Evaluating the predictions for the entropy code lengths is less complicated
since there's no logging involved. The predicted values are easily computed in
advance. To get the actual values, we simply encode the input file according to
the computed order until we have the MTF code, and then ask the entropy coder
for the code lengths for each MTF code.

The resulting pairs of actual and predicted values can be evaluated with the
same indicators as in the last section.

%TODO results for only first col and for all cols

%TODO update in case parameters to LKH change
\subsubsection{Feeding back correction values}

\subsection{Results}

%TODO in parentheses, give percentage point increase/decrease of compression
% compared to natural order
\begin{table}
\begin{tabular}{c|ccc|c|c}
\multicolumn{1}{c}{\rot{Metric}} & \multicolumn{1}{c}{\rot{weighted}} &
\multicolumn{1}{c}{\rot{entropy length prediction}} &
\multicolumn{1}{c}{\rot{MTF prediction}} &
\multicolumn{1}{c}{\rot{out size all columns}} & \multicolumn{1}{c}{\rot{out
size first columns}} \\ \hline
\multirow{30}{*}{Badness} & \ding{55} & \ding{55} & \ding{55} & 2139678 &
2136205 \\ \cline{2-6}
& \ding{55} & \ding{55} & generic mean & 2138206 & 2136016 \\ \cline{2-6}
& \ding{55} & \ding{55} & generic median & 2142001 & 2136027 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific mean & 2134375 & 2136170 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific median & 2138527 & 2136233 \\ \cline{2-6}
& \ding{55} & complete & \ding{55} & 2136792 & 2136132 \\ \cline{2-6}
& \ding{55} & complete & generic mean & 2134218 & 2135969 \\ \cline{2-6}
& \ding{55} & complete & generic median & 2136246 & 2136057 \\ \cline{2-6}
& \ding{55} & complete & specific mean & 2136068 & 2135825 \\ \cline{2-6}
& \ding{55} & complete & specific median & 2140188 & 2136234 \\ \cline{2-6}
& \ding{55} & sparse & \ding{55} & 2136801 & 2136132 \\ \cline{2-6}
& \ding{55} & sparse & generic mean & 2133870 & 2135927 \\ \cline{2-6}
& \ding{55} & sparse & generic median & 2134948 & 2136050 \\ \cline{2-6}
& \ding{55} & sparse & specific mean & 2134038 & 2135851 \\ \cline{2-6}
& \ding{55} & sparse & specific median & 2139348 & 2136143 \\ \cline{2-6}
& \ding{51} & \ding{55} & \ding{55} & 2135073 & 2135997 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic mean & 2133035 & 2135943 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic median & 2134234 & 2135925 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific mean & 2132082 & 2135981 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific median & 2140333 & 2135895 \\ \cline{2-6}
& \ding{51} & complete & \ding{55} & 2136417 & 2135969 \\ \cline{2-6}
& \ding{51} & complete & generic mean & 2134739 & 2135988 \\ \cline{2-6}
& \ding{51} & complete & generic median & 2134416 & 2136048 \\ \cline{2-6}
& \ding{51} & complete & specific mean & 2134474 & 2135831 \\ \cline{2-6}
& \ding{51} & complete & specific median & 2136421 & 2136078 \\ \cline{2-6}
& \ding{51} & sparse & \ding{55} & 2136388 & 2135969 \\ \cline{2-6}
& \ding{51} & sparse & generic mean & 2135061 & 2135922 \\ \cline{2-6}
& \ding{51} & sparse & generic median & 2134390 & 2136036 \\ \cline{2-6}
& \ding{51} & sparse & specific mean & 2135461 & 2135815 \\ \cline{2-6}
& \ding{51} & sparse & specific median & 2136453 & 2136061 \\ \hline
\multicolumn{4}{c|}{natural order} & 2136995 & 2136995 \\ \hline
\multicolumn{4}{c|}{aeiou} & 2132079 & 2136451 \\ \hline
\multicolumn{4}{c|}{Chapin hst diff} & 2134757 & 2136377 \\ \hline
\multicolumn{4}{c|}{Chapin inv} & 2134449 & 2136168 \\ \hline
\multicolumn{4}{c|}{Chapin inv log} & 2134371 & 2136131 \\ \hline
\end{tabular}
%TODO
\caption{Simulated compression results for book1 of the Calgary Corpus using
one order, in size 6150168}
\end{table}

\begin{table}
\begin{tabular}{c|ccc|c|c}
\multicolumn{1}{c}{\rot{Metric}} & \multicolumn{1}{c}{\rot{weighted}} &
\multicolumn{1}{c}{\rot{entropy length prediction}} &
\multicolumn{1}{c}{\rot{MTF prediction}} &
\multicolumn{1}{c}{\rot{out size all columns}} & \multicolumn{1}{c}{\rot{out
size first columns}} \\ \hline
\multirow{30}{*}{Badness} & \ding{55} & \ding{55} & \ding{55} & 147160 & 145360
\\ \cline{2-6}
& \ding{55} & \ding{55} & generic mean & 146329 & 145027 \\ \cline{2-6}
& \ding{55} & \ding{55} & generic median & 146346 & 145046 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific mean & 146075 & 145026 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific median & 146509 & 145088 \\ \cline{2-6}
& \ding{55} & complete & \ding{55} & 146995 & 145395 \\ \cline{2-6}
& \ding{55} & complete & generic mean & 145952 & 144928 \\ \cline{2-6}
& \ding{55} & complete & generic median & 146225 & 145067 \\ \cline{2-6}
& \ding{55} & complete & specific mean & 145671 & 144939 \\ \cline{2-6}
& \ding{55} & complete & specific median & 146124 & 145119 \\ \cline{2-6}
& \ding{55} & sparse & \ding{55} & 147000 & 145395 \\ \cline{2-6}
& \ding{55} & sparse & generic mean & 145519 & 144941 \\ \cline{2-6}
& \ding{55} & sparse & generic median & 145910 & 144995 \\ \cline{2-6}
& \ding{55} & sparse & specific mean & 146043 & 144966 \\ \cline{2-6}
& \ding{55} & sparse & specific median & 146904 & 145121 \\ \cline{2-6}
& \ding{51} & \ding{55} & \ding{55} & 146494 & 144961 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic mean & 145900 & 144853 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic median & 146071 & 144992 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific mean & 145637 & 144922 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific median & 145831 & 144882 \\ \cline{2-6}
& \ding{51} & complete & \ding{55} & 146354 & 144992 \\ \cline{2-6}
& \ding{51} & complete & generic mean & 145978 & 144798 \\ \cline{2-6}
& \ding{51} & complete & generic median & 145801 & 144976 \\ \cline{2-6}
& \ding{51} & complete & specific mean & 146815 & 145015 \\ \cline{2-6}
& \ding{51} & complete & specific median & 146061 & 144945 \\ \cline{2-6}
& \ding{51} & sparse & \ding{55} & 146354 & 144987 \\ \cline{2-6}
& \ding{51} & sparse & generic mean & 145976 & 144798 \\ \cline{2-6}
& \ding{51} & sparse & generic median & 145801 & 144976 \\ \cline{2-6}
& \ding{51} & sparse & specific mean & 146011 & 144959 \\ \cline{2-6}
& \ding{51} & sparse & specific median & 145862 & 144998 \\ \hline
\multicolumn{4}{c|}{natural order} & 145457 & 145457 \\ \hline
\multicolumn{4}{c|}{aeiou} & 144742 & 145378 \\ \hline
\multicolumn{4}{c|}{Chapin hst diff} & 145226 & 145237 \\ \hline
\multicolumn{4}{c|}{Chapin inv} & 145004 & 144970 \\ \hline
\multicolumn{4}{c|}{Chapin inv log} & 145613 & 145071 \\ \hline
\end{tabular}
%TODO
\caption{Simulated compression results for paper1 of the Calgary Corpus using
one order, in size 425288}
\end{table}

%-first column only, all columns, with feedback corrections, compare
%-chapin's metrics in my impl not quite as good as handpicked, same in his

book1, first col only:
\begin{itemize}
  \item first col only brings almost no benefit when using the handpicked order
  \item for first col only almost all badness variants perform better than
  handpicked or Chapin's
  \item with weighting is usually better
  \item when entropy code length prediction is turned on, it can affect the
  result in either direction, but usually not by much. possible explanation: its
  output are low integers instead of (higher) floats, this takes some precision
  away. when the prediction is correct for most values, it affects the result
  positively (as it's theoretically ideal), but if a couple of values are off by
  a bit, it introduces a considerable relative error (arithmetic coding?). also,
  they seem to be pretty useless on their own (without either mtf prediction or
  weighting).
  \item all kinds of mtf predicition have a positive influence, unless when
  combined with entropy length prediction, in which case results vary (gonna
  put this on entr len pred, see above). both generic and specific median give
  bad results when combined with entr len pred. this could be because the median
  tips the entr len predictor into incorrect results (some form of rounding
  error, see above) and so this could be reversed for a different file, but
  maybe not. generic prediction works much better when no weigthing occurs,
  otherwise the specific version is marginally better. no significant difference
  between mean and median, except for what's described before.
\end{itemize}

all cols:
\begin{itemize}
  \item when only considering first col reordering, almost all badness variants
  were better than handpicked and Chapin's, with all cols only a few. some are
  even worse than the standard ordering. so badness is good at what it was
  designed to do, but obviously not all variants make good reorderings for every
  column.
  \item when using all cols, much higher compression gain (up to 5x). this means
  reordering of subsequent cols to make 2nd- 3rd- etc order transitions better
  is very relevant compared to just considering the first column. this makes
  sense: compression is gained where transitions are reordered. if there are n
  transitions of the first order, there are up to \(n^2\) of the second, \(n^3\)
  of the third etc. but at a certain point the k-th order context only contains
  one or a few symbols, making reorderings not seem useful anymore. if the file
  is larger and contains more contexts, reordering further into the columns can
  become useful. (it becomes useless at the latest when all the columns in the
  bw table up to that column are unique)
  %TODO more?
\end{itemize}

%TODO update in case parameters to LKH change
%TODO more files, book2, some non-text

\section{Further Work}

%-sorting more useful if using a different list update algo? (with slower
%	convergence)
%-analyze longer transitions, i.e. not all a->b, a!=b, but all a->b->c, a!=b!=c
%	etc. reduces necessity for predictions, extreme case: try all possible
%	orders
%-bzip does bw block wise. this means more transitions. reordering more useful
%	there? maybe that's why chapin's results were better

\chapter{Ordering more columns independently}

%-transition from last 2nd-order context in first 1st-order context to first
% 2nd-order context in second 1st-order context relevant?

The previous chapter showed that, when only the first column of the BW table is
sorted according to the computed order and the rest as usual, there is much less
potential for compression improvement than if the order is used for all columns.

Furthermore, while the handpicked order and Chapin's metrics produce reliably
better results when used on all columns compared to only the first column,
results of the Badness metric are erratic: some variants are much better than
Chapin's metrics or the handpicked order, while some actually make compression
worse than the natural order although, when used only on the first column they
produce good results.

This makes to a point, since the Badness metric was designed for the very
specific purpose of finding an ideal reordering of blocks of MTF code (for
which only the order of the first column is changed), while Chapin's metrics
find more general similarities between blocks of BW code.

So in this chapter I try to make specific orders for more columns than just the
first to increase the compression gained from reordering.

\section{Generic and specific orders}
%TODO proper definitions: order, specific order (dictionary), prefix

During the sorting stage of the BWT, more than one sort order can be given. In
the simplest case they are all generic orders: The rows are first sorted by
their first symbol according to the first sort order given. Subsequent symbols
are used as tie breakers where the previous ones were all the same, with the
\(n\)-th symbols of a row being sorted according to the \(n\)-th order given.

To simplify, there don't need to be sort orders specified for each column; in
this case, the last order is used as the default for all following columns.

For example, the input \texttt{mississippi} sorted with the order \texttt{[i, m,
p, s]} for the first and \texttt{[s, p, m, i]} for all following columns would
yield the BW table

\begin{table}
\begin{tabular}{|r||ttttttttttt|}
\hline
0 & i & s & s & i & s & s & i & p & p & i & m \\
1 & i & s & s & i & p & p & i & m & i & s & s \\
2 & i & p & p & i & m & i & s & s & i & s & s \\
3 & i & m & i & s & s & i & s & s & i & p & p \\
4 & m & i & s & s & i & s & s & i & p & p & i \\
5 & p & p & i & m & i & s & s & i & s & s & i \\
6 & p & i & m & i & s & s & i & s & s & i & p \\
7 & s & s & i & s & s & i & p & p & i & m & i \\
8 & s & s & i & p & p & i & m & i & s & s & i \\
9 & s & i & s & s & i & p & p & i & m & i & s \\
10 & s & i & p & p & i & m & i & s & s & i & s \\
\hline
\end{tabular}
%TODO
\caption{example table}
\end{table}

and the BW code \texttt{msspiipiiss}.

The more complex case is that of specific orders. Here, for the \(n\)-th column,
multiple orders are given, one for each subsequence of symbols of length \(n -
1\) that is the beginning of a row in the BW table (the prefix). The order
for the first column is of course still a generic one, since there is only one
prefix of length zero.

For example, suppose the strings \texttt{aa}, \texttt{ab}, \texttt{ba} and
\texttt{bb} appear at the beginning of rows in a BW table. The order for the
first column shall be \texttt{[a, b]}. There are two prefixes of length \(1\),
namely \texttt{a} and \texttt{b}. The orders for the second column shall be
\texttt{[a, b]} for prefix \texttt{a} and \texttt{[b, a]} for prefix \texttt{b}.
They would be sorted as follows:

\begin{tabular}{c}
\texttt{aa} \\
\texttt{ab} \\
\texttt{bb} \\ 
\texttt{ba} \\
\end{tabular}

If the default order is a specific one, it has to be used for columns with depth
greater than the prefix length of the order. In this case, the symbols
immediately preceding the symbol to be sorted must be used as a prefix (instead
of the symbols at the beginning of the row). This is important to allow
reversibility (see next section).

Using specific orders, the transitions within different contexts can be
optimized separately. For example, the transition in the BW code from
\texttt{aa} to \texttt{ab} may be good for compression while the transition
from \texttt{xa} to \texttt{xb} isn't especially good or even bad. Giving
specific orders for 2 columns allows to sort \texttt{b} after \texttt{a} if
they're preceded by \texttt{a}, but sort them differently when they're preceded
by something else.

Providing specific orders for multiple columns increases the amount of
transitions that can be optimized: If the input has \(n\) distinct symbols,
there are \(n - 1\) transitions in the first column. In the second column, there
are up to \(n - 1\) transitions in the context of each of the \(n\) symbols. In
general, if there are special orderings for \(k\) columns given, there are up to
\(\sum_{i=0}^{k} n^{i} \cdot (n - 1)\) transitions. ``Up to'', because not every
symbols has to appear after every other symbol. In fact, the further you get
into the columns, the bigger the difference between the upper bound and the
actual number of encountered transitions will be. The individual context blocks
will of course also get smaller.

%-overhead

\section{Reversibility}

In order for the BWT with multiple sort orders to be useful, the transformation
needs to be reversible.

I can show that it is, if only two sort orders are given (the second one can be
specific). I have, however, been unable to show reversibility for an arbitrary
number of sort orders. But I believe it is possible and will give my unfinished
algorithm and point out the part where it fails in some cases.

\subsection{Two Orders}

As with the regular BWT with only one order, the BW code (the last column of
the BW table), the index of the first symbol in it and all the orders are given
to the decoder. The first column of the BW table can be reconstructed by sorting
the code according to the order for the first column. It is then easy, for any
given index in the code to give the symbol that follows it.

The tricky part is still to match an index in the first column to an index in
the last column to continue decoding. With only one order, the \(i\)-th
occurrence of a symbol in the first column would match the \(i\)-th occurrence
of that symbol in the last column because symbols that compare equal are sorted
by the sequences of symbols that follow, and the rows where that symbol is in
the last column are sorted by the same sequences.

But with two orders, the sequences are sorted differently when they begin in the
first column than when they begin in the second since the sort order for the
first column is different from that of the other columns.

This problem can be solved by ``looking ahead'' one more symbol and reordering
accordingly. The following is the algorithm to match an index in the first
column to an index in the last column.

\begin{algorithmic}[1]
\Procedure{next\_index}{$idx$, $first\_col$, $last\_col$, $second\_order$}
\State $sym \gets first\_col[idx]$
\State $num \gets$ number of appearances of $sym$ in $first\_col$ with index $<
idx$
\State $possible\_idx \gets [i | last\_col[i] = sym]$\Comment{this list must be
sorted}
\State sort $possible\_idx$ according to $second\_order$, using $first\_col[i]$
as the key for element $i$\Comment{this must be a stable sort}
\State \Return $possible\_idx[num]$
\EndProcedure
\end{algorithmic}

%TODO line 5 only thing different from normal BW
In line 5, %TODO correct line?
the possible indices are sorted as though the second order was used for the
first column. Since all subsequent orders are the same (there are only two),
this means the sequences at the beginning of the rows with those indices are in
the same order as when they appear starting in the second column. We now have
the same situation as if we were reversing a BWT with only one order and can
return the appropriate index.

%TODO specific order, why it's important default order uses symbol directly
% before

\subsection{More Orders}

I have tried to modify the algorithm to work with more orders, but what I have
so far can potentially create a livelock.

The algorithm doesn't return only one index, but a sequence of indices. During
execution, many possible sequences of indices may have to be saved to
determine the correct one, so this is, first of all, an optimization -- if we
have all those correct indices, why only return one? But it can also become
necessary for correctness to return more than one index if there are multiple
possible index sequences that would decode the whole file. They all give the
correct result, but returning only the first index of one of them might make it
impossible to correctly continue the sequence with the next call to the
function.

\begin{algorithmic}[1]
\Procedure{next\_indices}{$idx$, $first\_col$, $last\_col$, $orders$, $history$}
\State $sym \gets first\_col[idx]$
\State $num \gets$ number of appearances of $sym$ in $first\_col$ with index $<
idx$
\State $possible\_idx \gets [([i], \True) | last\_col[i] = sym]$
\EndProcedure
\end{algorithmic}

\section{Computing the Orders}

Actually computing specific orders for more than one column is fairly
straightforward: First, the order for the first column is computed as in the
previous chapter. The \(n\)-th specific order is a collection of orders, one for
each subsequence of length \(n - 1\). For each of these subsequences, the
context block is extracted from the BW code and the order computed as if it were
the first order.

%TODO describe both approaches, describe why i use the bad one

Unfortunately, with this approach the orders that are computed first are likely
to get messed up again, as during calculating the \(n\)-th order context blocks
of the \(n + 1\)-st order are assumed to be in an order that is likely to change
when the \(n + 1\)-st order is computed.

The alternative would be to first compute all the highest level orders, so that
the transitions ``further into the table'' are already done and the lower next
order can be computed with the correct higher level transitions. 

\section{Results}

\begin{table}
\begin{tabular}{c|ccc|c|c}
\multicolumn{1}{c}{\rot{Metric}} & \multicolumn{1}{c}{\rot{weighted}} &
\multicolumn{1}{c}{\rot{entropy length prediction}} &
\multicolumn{1}{c}{\rot{MTF prediction}} &
\multicolumn{1}{c}{\rot{out size all columns}} & \multicolumn{1}{c}{\rot{out
size first columns}} \\ \hline
\multirow{30}{*}{Badness} & \ding{55} & \ding{55} & \ding{55} & 2140704 &
2132844 \\ \cline{2-6}
& \ding{55} & \ding{55} & generic mean & 2137932 & 2131678\\ \cline{2-6}
& \ding{55} & \ding{55} & generic median & 2140252 & 2132114 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific mean & 2139041 & 2131493 \\ \cline{2-6}
& \ding{55} & \ding{55} & specific median & 2139429 & 2132034 \\ \cline{2-6}
& \ding{55} & complete & \ding{55} & 2142693 & 2132211 \\ \cline{2-6}
& \ding{55} & complete & generic mean & 2141060 & 2130990 \\ \cline{2-6}
& \ding{55} & complete & generic median & 2141739 & 2131500 \\ \cline{2-6}
& \ding{55} & complete & specific mean & 2139324 & 2130702 \\ \cline{2-6}
& \ding{55} & complete & specific median & 2138707 & 2131341 \\ \cline{2-6}
& \ding{55} & sparse & \ding{55} & 2142703 & 2132200 \\ \cline{2-6}
& \ding{55} & sparse & generic mean & 2140673 & 2130893 \\ \cline{2-6}
& \ding{55} & sparse & generic median & 2141777 & 2131554 \\ \cline{2-6}
& \ding{55} & sparse & specific mean & 2140004 & 2130585 \\ \cline{2-6}
& \ding{55} & sparse & specific median & 2138926 & 2131382 \\ \cline{2-6}
& \ding{51} & \ding{55} & \ding{55} & 2140731 & 2131540 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic mean & 2138945 & 2131045 \\ \cline{2-6}
& \ding{51} & \ding{55} & generic median & 2140855 & 2131261 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific mean & 2139157 & 2131166 \\ \cline{2-6}
& \ding{51} & \ding{55} & specific median & 2138515 & 2131052 \\ \cline{2-6}
& \ding{51} & complete & \ding{55} & 2140784 & 2130963 \\ \cline{2-6}
& \ding{51} & complete & generic mean & 2138936 & 2130666 \\ \cline{2-6}
& \ding{51} & complete & generic median & 2138882 & 2130715 \\ \cline{2-6}
& \ding{51} & complete & specific mean & 2137581 & 2130521 \\ \cline{2-6}
& \ding{51} & complete & specific median & 2138855 & 2130707 \\ \cline{2-6}
& \ding{51} & sparse & \ding{55} & 2140777 & 2130958 \\ \cline{2-6}
& \ding{51} & sparse & generic mean & 2138930 & 2130582 \\ \cline{2-6}
& \ding{51} & sparse & generic median & 2138882 & 2130715 \\ \cline{2-6}
& \ding{51} & sparse & specific mean & 2138153 & 2130583 \\ \cline{2-6}
& \ding{51} & sparse & specific median & 2138909 & 2130750 \\ \hline
\multicolumn{4}{c|}{natural order} & 2136995 & 2136995 \\ \hline
\multicolumn{4}{c|}{aeiou} & 2132079 & 2136451 \\ \hline
\multicolumn{4}{c|}{Chapin hst diff} & 2139151 & 2135040 \\ \hline
\multicolumn{4}{c|}{Chapin inv} & 2135684 & 2133483 \\ \hline
\multicolumn{4}{c|}{Chapin inv log} & 2136573 & 2133527 \\ \hline
\end{tabular}
%TODO
\caption{Simulated compression results for book1 of the Calgary Corpus using
two orders, in size 6150168}
\end{table}

\section{Further Work}

%-selecting only a limited number of distinct orders for more efficient format
%-maybe make a selection of frequently useful orders that can be accessed in the
%	decoder

\chapter{Exceptions to MTF}

%-when removing blocks from the bw, transitions are ripped out and  the computed
% order isn't correct anymore
%-bzip2 has the ability to switch between multiple huffman tables. maybe that
% makes exceptions to mtf superfluous
%-adaptive entropy coder might also limit the problem
%-when encoded with mtf, no information about underlying symbols remains,
% frequencies can't be exploited as well

This chapter doesn't have a strong relation to the previous chapters. It's about
how some parts of the BW code don't contextualize very well and therefore aren't
very suitable for encoding using MTF. Excepting unsuitable contexts from the MTF
phase, and instead compression them directly with an entropy coder, can increase
overall compression.

\section{Related Work}

adaptive coders, fenwick noticed that arithmetic coders without a sliding window
were performing worse because frequencies of mtf change\cite{fenwick1996block}

gagie, manzini overview of different list update algos that have been
examined\cite{gagie2007listupdate}

chapin tried using two list update algos
\cite{chapin2000switching,chapin2001diss}. also assumes that one algo isn't
universally the best

wirth, moffat, altered or no list update \cite{wirth2001ranks}

\section{The Problem}

%TODO entropy coder -> Huffman

In the BW table, rows that begin with the same symbols are sorted one below the
other, with the symbols immediately preceding them forming part of the BW code.
In input files that are suitable for compression, the same sequences are likely
to be preceded by a symbol from a small set of likely symbols, and so the BW
code contains blocks in which only a small number of distinct symbols appear,
with long runs of the same symbol.

For example, consider rows beginning with ``\texttt{nd }'' in the BW table of an
English language text file. The BW code corresponding to these rows is likely to
contain many \texttt{a}'s and \texttt{A}'s for the word \emph{and}, but also
some \texttt{e}'s (\emph{end}) or \texttt{u}'s (\emph{found}).

Long runs of the same symbol in the BW code mean long runs of zeros in the MTF
code. And if the current symbol in the BW code changes to one of the other
likely symbols, it is likely to be close to the front of the MTF alphabet,
meaning a low code will follow.

When the resulting MTF code is passed to the (static) entropy coder, it
recognizes that very low MTF codes (and especially 0) appear much more
frequently than others and assigns them much shorter codes.

But now consider that not all symbols contextualize well, i.e. that the BW code
generated by the rows that begin with that symbol doesn't contain long runs and
doesn't only contain symbols from a small set.

For example, consider rows beginning with ``\texttt{. }''. These typically mark
the end of a sentence and are followed by the next sentence. The symbols in the
corresponding BW code would each be the last symbol of the last word of the
sentence. They would be symbols that are likely to appear at the end of a
word. But the way the next sentence starts doesn't really give any additional
information, as it doesn't say anything about what the last symbol of the
previous sentence was.

If a BW code like this is encoded with MTF, there won't be any long runs of
zeros and the codes will on average be much higher than in most of the other MTF
code. The code will be erratically jumping, as there is no real connection
between consecutive rows in the BW table.

But since the entropy coder considers the whole MTF code to make the code
book and assigns very short codes to low MTF codes, it also assigns very long
codes to higher MTF codes. This means that this MTF code is encoded with some
very long entropy codes and will get rather big.

On top of that, even if there are still only few high MTF codes, their slightly
increased frequency might actually affect the statistical analysis of the
Huffman coder: If there are more high codes, they will get shorter Huffman
codes, causing some of the lower codes to get longer ones.
%TODO not clear how this is a problem

\section{The Solution}

Symbols that don't contextualize well are not suited for the MTF phase. They
should skip that phase and be encoded with an entropy coder separately.

Looking at the ``\texttt{. }'' example again: The order in which the symbols in
the BW appear is not related to the beginning of the row and so the MTF code
would be erratic. But they are all symbols appearing at the end of words, and so
some symbols are more likely to appear than others -- a perfect candidate for
entropy coding.
%TODO format, overhead

\subsection{Selecting symbols to be excepted}

In order to find symbols that don't contextualize well and should be excepted
from the MTF phase, first do a regular BWT and MTF encode. For each distinct
symbol in the input, cut out the MTF code corresponding to the BW code from the
rows starting with that symbol. Calculate the average of the codes. If it is
above a threshold, consider the symbol not suitable for MTF. Additionally,
require the MTF code to have a certain length to be excepted, in order not to
let the overhead from the special treatment destroy the gains.

\section{Results}

\section{Future Work}

%-except smaller contexts, e.g. '. ' instead of '.'

\bibliographystyle{plain}
\bibliography{bib}
\end{document}
