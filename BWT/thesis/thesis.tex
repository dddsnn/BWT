\documentclass[a4paper]{scrreprt}
\usepackage{multirow}
\usepackage{pifont}

\begin{document}

\chapter{Intro}

%-def compression (decompression necessary)
%-explain bw-mtf-entropy, mtf can be skipped
%-explain simple version of bw used here, no rle
%-definitions: context block, symbol (byte in this thesis), natural sort order,
%	transition

\chapter{Simple Sorting}

%-overwork in context transitions
%-metrics to rank transitions, tsp to find a tour
%-problem finding atsp heuristic, only one i found only supports ints, give 
%	error bounds for the scaling
%-chapins metrics work on the bw code
%-badness works on the mtf of the bw, with modifications attempts to give the
%	number of bits a transition costs:
%	-make a list of the best possible alphabet the left side of a transition
%		can leave for the right side
%	-compare those with the actual codes you get when you mtf encode the left
%		and right side together
%	-the sum of all the differences is the badness
%	-variants:
%		-weighting: divide the badness by the number of new symbols on the right
%			side, so transitions to a larger context don't get a massive
%			penalty
%		-new penalty: new symbols in the right side that didn't appear in the
%			left side need to receive a penalty instead of assuming the minimum
%			possible (mean mtf code for mtf codes of at least the min possible,
%			symbol specific)
%			prediction for the block that ends up as the first will be way off
%		-entropy code len: don't take the plain difference but the difference
%			between the number of bits the entropy coder will use. this can
%			give you the exact number of bits this transition costs
%			approximation with const values for zeros: zeros will all get the
%			same (longest possible) code length, other rare symbols may be
%			affected
%			approximation with curve fitting: zeros will get about the same
%			code lengths as the surrounding symbols, but all of the rarer
%			symbols (>10?) will have (significantly) longer codes as there are
%			more symbols to encode. higher value mtf codes are usually sparse,
%			but can't be sure how many zeros will remain after reordering
%			curve fitting sparse: assume there will be the same amount of
%			zeros after reordering. give zeros the same length as their
%			neighbors, even though there can't be a huffman code with those
%			code lengths and that many symbols 
%	-describe things needed for an ideal badness metric (new penalty,entropy
%		len, weighting?), then give approximations
%	-another point of non-optimality: which transition to choose as a starting
%		point? need to break up the baddest transition
%	-need need a new penalty dict that is specific for the symbol being encoded
%		i.e. what is the average code for e.g. 'a' given that the minimum
%		possible is n
%	-new penalty predictor that gives the average code for a specific symbol
% 		that is new in the block, with a given mimimum
%	-much higher compression benefit when sorting all columns instead of just
%		the first one -> a lot of the compression benefit comes from the higher
% 		order transitions
%	-after comparing predictions with actual values, do another pass of
%		calculating transitions, but pass a compensation summand that will be
%		added to every prediction, so that the average is 0

The reason Burrows-Wheeler based compression algorithms are so effective is that
the Burrows-Wheeler Transform (BWT) creates sequences of symbols that appear in
the same context. If the input file is suitable for context-based
compression, these sequences will consist of only a few distinct symbols with
lots of repetition, leading to many small numbers in the MTF code.

But any time the context changes, in the Burrows-Wheeler code a new sequence
begins that is in general not related to the previous one. This incurs some
overwork\cite{bitner1979heuristics} for the MTF coder as it has to adapt to the
new context, resulting in larger codes for symbols that were not requested in
the previous sequence.

%TODO why doesn't this affect decompressibility
%TODO overhead needed to store reordering

The idea behind using different sort orders is to order the contexts in a way
that reduces the overwork caused by a transition from one context to the next,
resulting in lower MTF codes and higher compression.

In order to find a suitable reordering, you need to do the BWT on the input data
to get the context blocks, rank all possible transitions between blocks with a
cost metric and find a sort order based on the costs. Finding the ordering with
given costs is an instance of the traveling salesman problem, where the nodes
are the context blocks and the distances between them are the cost of the
transition.

\section{Presious Work}

chapin original \cite{chapin1998sort,chapin2001diss}
merge with next section unless there's more

\section{Chapin's Metrics}

%TODO aeiou

Chapin uses four different cost metrics, three of which I have reimplemented for
comparison with my own metric.\footnote{The fourth uses the Kullback-Leibler
distance, but is unclear about how to deal with zeros in the PMF.} All of them
analyze the histograms of symbol appearances for every context block and try to
give a measure of how similar they are. That means: For each context block
(generated by BW encoding the input with the natural sort order), make a
histogram of the number of symbol appearances in that block, i.e.
a mapping from each possible symbol to the number of times it appears in that
block.

In Chapin's first metric, two histograms are compared by taking the
logarithm\footnote{Chapin doesn't specify to which base. In my implementation,
I use the natural logarithm.} of all of the symbol counts, calculating, for each
symbol, the differences between the logarithms in both histograms and summing up
the squares of all of the differences. The result is used as a measure of
similarity between the two contexts.

For the second metric, the symbol counts from the histograms are written to a
list in decreasing order. The cost of a transition is the number of
inversions\cite{sleator1985amortized} between the two corresponding lists.

The third metric is just the logarithm of the second metric.\footnote{Again, no
base was specified, I use the natural logarithm.}

-not comparable with my data because of variations in tsp and the actual
compression algo

\section{The Badness Metric}

My own metric attempts to give a value denoting how bad any given transition is
and is hence called the badness metric.\footnote{Only after I had given it that
name and used it all over my code did it come to my mind that assigning a
badness value is really the purpose of any such metric.} Other than Chapin's
metrics, it doesn't operate on the BW code, but on the MTF code it generates
and so, in a sense, is ``closer to the compression''.


\subsection{Ugly LKH}

The badness values generated are not necessarily symmetric, i.e. the badness of
the transition from context x to y is not necessarily the badness of y to x.
This has the unfortunate consequence that I now need a heuristic solver for the
asymmetrical traveling salesman problem, which is much harder to find than a
heuristic for the ``ordinary'' symmetrical TSP. I finally found
LKH\cite{helsgaun2000lkh}\footnote{http://www.akira.ruc.dk/~keld/research/LKH/},
which can approximate solutions to the asymmetrical TSP. Unfortunately, it can
not handle floating point input, so I have to do some ugly scaling.

The transition data is exported as a .atsp file in full matrix format according
to the TSPLIB specification\cite{reinelt1991tsplib}. LKH doesn't allow floating
point numbers in the matrix, so the original values have to be converted into
integers while preserving the ratios between all of the values as best as
possible. Since just rounding to the nearest integer would incur large relative
errors for values close to zero, they are first scaled up. If all values are
multiplied with the same factor, the solution to the TSP remains the same. This
factor is the greatest number such that the absolute greatest original value,
when multiplied with the factor, is less than or equal to a preset maximum. If
this maximum is too large, LKH will fail an assertion, probably because the
variable overflows. I have chosen \(10^7\), as this seems to be small
enough to not cause problems.
%TODO still 2*10^7?

To get a picture of the accuracy of this transformation, for every pair of
values the ratio of the original (floating-point) values and the ratio of their
correspoding integers is computed and the relative error calculated. The maximum
relative error for the examples is rarely greater than \(10^-4\).
%TODO update in case this changes
LKH is run with default parameters, with the exception of the number of runs,
which is 100 rather than the default 10.

\subsection{Basic Operation}

To explain the badness metric, I need to introduce the partial MTF code that
will be used on appropriate parts of the BW code. This is not absolutely
necessary to get the actual badness value, but useful to understand why the
metric does what it does.

The partial MTF differs from the regular MTF code only in that you start with an
empty alphabet and encode an escape code (I will use -1) anytime a symbol
appears that isn't already in the alphabet.
E.g., the ASCII encoded string ``aabcab'' encoded (byte-wise) with regular MTF
would yield \([97, 0, 98, 99, 2, 2]\), encoded with partial MTF it would be
\([-1, 0, -1, -1, 2, 2]\).

%TODO property non-escape codes are the same

Of course this means that it can't be decoded anymore, but we only need it to
illustrate the operation of the metric.

To compute the badness value of a transition, the metric needs the context
blocks for both sides of the transition, i.e. the BW code belonging to the
symbol that's being transitioned from or to, respectively.

It then produces the partial MTF code of the destination block (the right
side), and of the concatenation of the source and destination block (combined
partial MTF).
So it  basically pretends that the two blocks were sorted one after the other,
and then attempts to give an indication of how bad this would be for the
compression.

Looking at the partial MTF code of the destination side of the transition, the
metric assumes that all recurring codes (i.e. those that aren't escape codes)
are not interesting for the purposes of ranking transitions, as they would be
the same no matter what comes before or after.\footnote{This assumtion is not
actually correct, as I will explain in the next section.}

What is of interest, are the escape codes on the right side of the transition,
because this is where what has previously been encoded can influence the MTF
codes in the final result. When the input file is finally encoded with the
(full) MTF, each of the escape symbols will be replaced by an actual code. How
high those codes are, determines the compression lost by the transition between
contexts.

The metric compiles the ideal MTF alphabet the left side can ``leave'' for the
right side.

E.g., the first code in the partial MTF of the right side is necessarily an
escape code. It would be ideal, if the last symbol of the left side were the
first symbol of the right side. That would mean that it is at the front of the
alphabet when the transition occurrs, and so that first escape code would, in
the full MTF be encoded as a 0 (and smaller codes are better). The ideal full
MTF code for the next escape code is 1 (since the code 0 in the alphabet has
already been taken by whatever symbol came before it) etc.
%example? pseudo code?

The metric can then compare the ideal codes with the actual codes in the
combined partial MTF at the positions where the escape symbols are in the
partial MTF of just the right side. The differences between actual code and
ideal code are summed up to form the badness of the transition.

What can also happen is that the part of the combined partial MTF belonging to
the right side still contains escape codes. This means that the symbols encoded
by them do not appear in the left side of the transition at all. In this case,
the metric assumes the best possible code for that symbol, which is equal to the
number of escape symbols in the combined partial MTF up to that point (i.e. the
metric assumes that whatever context block ends up preceding the left side
of this transition will leave an ideal alphabet).

\subsection{Ideality, under some circumstances}

The metric assumes that the partial MTF codes of the right side of the
transition that are not escape codes don't change no matter the order that is
finally selected, and thus aren't interesting. This assumption is incorrect
since, when a different reordering is selected, the BW code of the context
blocks is also reordered, and so the generated MTF code will (probably) be
different. For Chapin's metrics, this doesn't matter because they only care
which symbol occurrs how often, not in which order.

So we'll consider a slightly different problem than finding an ideal global
reordering: instead of looking for a sort order by which the entire BW table
will be sorted, we look for an order by which only the first column of the BW
table will be sorted, all other columns will use the natural order (which was
used to generate the context blocks that are fed to the metric).

This is equivalent to finding a reordering of the fixed context blocks the
metric deals with. This means that recurring symbols in the partial MTF never
change, no matter what order is chosen for the first column, and so the
assumption the metric makes is true.

Let's further assume that we have two oracles that know which reordering will
finally be selected. When the metric encounters an escape symbol in the right
side of the combined partial MTF (i.e. a symbol that appears in the right side
doesn't appear in the left side), it has to assume the best possible code for
lack of information. The first oracle can provide the actual codes that will be
at those positions in the final computed order.

The second oracle can provide the codeword lengths in bits of the entropy coder
for every MTF code appearing in the final reordered and MTF encoded file.

If the badness metric is modified to use the predictions of the MTF code oracle
and sums up the differences between the entropy codeword lengths of the actual
and ideal MTF codes instead of the differences between the codes themselves, it
is the ideal metric for generating a reordering for only the first column.

Since only the first column gets reordered, recurring symbols within a
context block are fixed and have no influence, good or bad, on the compression
rate. The only thing that can make a difference are the MTF codes of symbols
occurring for the first time in the block. The badness metric records the
difference between the number of bits that are used if the left side of the
transition leaves an ideal alphabet for the right side, and the number of bits
that are actually used.

That way, if the transition (a,b) has a badness of n, and the transition (a,c)
has a badness of n+m, all other transition's badness values being equal, if the
transition (a,c) is used, the result will be m bits larger than if the
transition (a,b) was used.

If the badness values of all possible transitions are provided as input to an
exact TSP solver, it will generate the best possible reordering for only the
first column of the BW table.

\subsection{Variants}

There are three variants of the badness metric, two of which are attempting to
approximate the predictions of the oracles from the previous section.

\subsubsection{Weighting by number of symbols}

The computed badness value is divided by the number of distinct symbols in the
right side of the transition (i.e. the number of escape codes in the partial
MTF). This is supposed to counteract the effect where transitions whose right
side has many differenct symbols get a much worse rating than if they have less,
because every escape code leads to some badness being added to the total value.
The resulting value can be characterized as badness per distinct symbol.

\subsubsection{Predicting code lengths of the entropy coder}

%TODO: arithmetic coding? otherwise just call it Huffman coding
This modification tries to approximate the behavior of the oracle predicting the
code lengths of the entropy coder.

When adding to the badness value, instead of adding the difference between the
actual and the ideal MTF codes, the difference between a prediction of the
entropy code lengths of the actual and ideal MTF codes is added.

Getting pretty good predictions for a static entropy coder is relatively easy:
since different reorderings only has a little effect on the produced MTF code,
the entropy code lengths for any reordering should be within a small margin of
error of each other. In the case of static Huffman coding, many of the codes for
low MTF codes may not change at all.

So in order to get the predictions, the input file is simply BW- and MTF encoded
with the natural sort order and the lengths of the codewords are recorded that
the entropy coder would use for each MTF code.

There is a problem with this simple method when not all possible MTF codes
appear in the code that is used to make the predictions. This is usually the
case with e.g. ASCII text files, which tend to use less than 100 distinct
symbols\footnote{book1 from the calgary corpus uses 82 different characters:
upper- and lowercase alphabet, 10 digits and some punctuation and control
characters.}, so MTF codes above that number only occur once when that symbol is
fetched to the front of the MTF alphabet, and may not occur at all.
When a different reordering is chosen, a different MTF alphabet will probably be
in place at the time such a symbol is requested, and the resulting MTF code may
be shifted a little. That means that an MTF code for which a prediction exists
may not be requested at all, but a code that is requested has no prediction.

To solve this, I have developed two strategies, ``complete'' and ``sparse''
predictions.
The ``complete'' method modifies the symbol frequencies of the MTF code that are
used as weights by the entropy coder. Every MTF code that doesn't appear but is
smaller than the maximum code that does gets the weight \(\frac{1}{256}\). In
the case of Huffman coding this means that all these codes get a longer or
equally long entropy code than the codes that actually appear.

All other MTF codes that don't appear, but are greater than the maximum code
that does get weight \(\frac{1}{256^2}\), so their entropy codes are guaranteed
to be longer or equal to the ones with weight \(\frac{1}{256}\). The reasoning
behind this is that these codes are unlikely to appear in any reordering, e.g.
when encoding an ASCII text file, no codes above 127 will appear.

The ``sparse'' method assumes that no matter the reordering, there will always
be (high) MTF codes that don't appear (the histogram of MTF codes will be
sparse for high values).
It lets the entropy coder compute the codeword lengths with unaltered weights,
but inserts entropy code lengths for the missing MTF codes manually afterwards.
Each missing MTF code is assigned the same code length as the next smaller MTF
code that does appear. The reasoning is that, when MTF codes appear for one
reordering that didn't for another, they're just shifted around from codes that
don't appear anymore. Of course this means that there isn't actually an entropy
code with the code lengths that were predicted (since the predictor has
manually introduced collisions).

\subsubsection{Predicting MTF codes for new symbols}

This modification aims at approximating the oracle that can predict MTF codes of
escape codes in the right side of the combined partial MTF of a transition. When
the metric encounters an escape code, instead of assuming that whatever
context came before left an ideal alphabet, it can use the prediction, which is
hopefully more accurate.

Making these predictions is more complicated than those for the entropy code
lengths. I have again developed two strategies, ``generic'' and ``specific'',
which assume that the distribution of MTF codes will be similar no matter the
order.

The generic predictor does a BW encode on the input file with the natural order,
then encodes this with MTF. It then records, for every possible MTF code, the
average value of all MTF codes that are greater or equal to it.

The specific predictor also takes into account the underlying BW code. So it
records, for every possible MTF code and every possible symbol, the average
value of the MTF codes that are greater or equal to it and that encode the
underlying symbol.

The averaging function for both predictors can be the arithmetic mean or the
median.

\subsection{Evaluating the performance of the predictors}

Besides just trying out how much of an impact the different predictors have on
the final comression rate, we can also evaluate their performance by simply
comparing the values they predicted with the values that actually appear in the
reordered MTF code.

\subsubsection{MTF predictor}

For this purpose, every prediction of the MTF code predictor is logged while the
metric is computed. It is recorded in which transition the prediction happened,
which symbol of the underlying BW code is encoded by the MTF code that has to be
predicted, and the predicted value itself.

Once the TSP heuristic has computed the reordering according to the metric,
only the predictions of the transitions that occur in the reordering can be
evaluated. The predictions for the first context block in the reordering also
can't be evaluated, since predictions only happen in the right side of a
transition, and the first block is not the right side of any transition that
appears in the reordering.

When we have all the pairs (actual value, predicted value), we can calculate
indicators that can hint at the quality of the predictions:
\begin{itemize}
  \item The mean difference, i.e. the mean over all the (actual - predicted)
  pairs
  \item The mean deviation, i.e. the same as the mean difference, only with the
  absolute of the difference
  \item The (kind of) standard deviation, i.e. the square root of the mean of
  the squared differences between actual and predicted value.\footnote{This is
  not a real standard deviation, because we don't measure it with the distances
  from a single expected value, since each prediction gets its own actual
  value, the distance to which matters for the deviation. It's also not clear
  that the distances follow a normal distribution, and the distribution is much
  more spread out when the predicted value is less than the actual value than
  when it is greater.}
\end{itemize}

The mean difference can tell us how well the predictions are on average and
should be close to zero. If it is significantly greater or less, it means that
the predictor is too optimistic or pessimistic, respectively.

The deviations can tell us how far the predictions are spread around the correct
value, and should be as low as possible.

We can also calculate these indicators with just a subset of the
(predicted, actual) pairs:
\begin{itemize}
  \item We can exclude pairs where the actual value is
  greater than the number of distinct symbols in the input file. They only occur
  when a symbol that's never been encoded before needs to be fetched to the
  front of the MTF alphabet. They occur at most as many times as there are
  distinct symbols in the input, but they skew the averages significantly.
  \item We can only take pairs where the prediction is either greater than or
  equal, or less than the actual value. With this we can learn how far spread
  out the distribution is to either side.
\end{itemize}

Finally, we can calculate all these indicators while not using the difference
between the codes themselves, but between their entropy code lengths. This can
give us an indication of how big the error is in bits a prediction is likely to
cause.

\subsubsection{Entropy length predictor}

Evaluating the predictions for the entropy code lengths is less complicated
since there's no logging involved. The predicted values are easily computed in
advance. To get the actual values, we simply encode the input file according to
the computed order until we have the MTF code, and then ask the entropy coder
for the code lengths for each MTF code.

The resulting pairs of actual and predicted values can be evaluated with the
same indicators as in the last section.

%-TODO results for only first col and for all cols

TODO update in case parameters to LKH change
\subsubsection{Feeding back correction values}

\subsection{Results}

book1
\begin{tabular}{|c|c|c|c|c|}
\hline
Metric & weighted & entropy length prediction & MTF prediction & out size all
columns \\ \hline
\multirow{30}{*}{Badness} & \ding{55} & \ding{55} & \ding{55} & 2139673 \\
\cline{2-5} & \ding{55} & \ding{55} & generic mean & 2138203 \\ \cline{2-5}
& \ding{55} & \ding{55} & generic median & 2142013 \\ \cline{2-5}
& \ding{55} & \ding{55} & specific mean & 2134390 \\ \cline{2-5}
& \ding{55} & \ding{55} & specific median & 2138527 \\ \cline{2-5}
& \ding{55} & complete & \ding{55} & 2139013 \\ \cline{2-5}
& \ding{55} & complete & generic mean & 2133935 \\ \cline{2-5}
& \ding{55} & complete & generic median & 2135820 \\ \cline{2-5}
& \ding{55} & complete & specific mean & 2135364 \\ \cline{2-5}
& \ding{55} & complete & specific median & 2140979 \\ \cline{2-5}
& \ding{55} & sparse & \ding{55} & 2139013 \\ \cline{2-5}
& \ding{55} & sparse & generic mean & 2133644 \\ \cline{2-5}
& \ding{55} & sparse & generic median & 2135622 \\ \cline{2-5}
& \ding{55} & sparse & specific mean & 2136052 \\ \cline{2-5}
& \ding{55} & sparse & specific median & 2138426 \\ \cline{2-5}
& \ding{51} & \ding{55} & \ding{55} & 2135080 \\ \cline{2-5}
& \ding{51} & \ding{55} & generic mean & 2133012 \\ \cline{2-5}
& \ding{51} & \ding{55} & generic median & 2134234 \\ \cline{2-5}
& \ding{51} & \ding{55} & specific mean & 2132082 \\ \cline{2-5}
& \ding{51} & \ding{55} & specific median & 2140333 \\ \cline{2-5}
& \ding{51} & complete & \ding{55} & 2136412 \\ \cline{2-5}
& \ding{51} & complete & generic mean & 2134734 \\ \cline{2-5}
& \ding{51} & complete & generic median & 2134390 \\ \cline{2-5}
& \ding{51} & complete & specific mean & 2134469 \\ \cline{2-5}
& \ding{51} & complete & specific median & 2136416 \\ \cline{2-5}
& \ding{51} & sparse & \ding{55} & 2136417 \\ \cline{2-5}
& \ding{51} & sparse & generic mean & 2135061 \\ \cline{2-5}
& \ding{51} & sparse & generic median & 2134390 \\ \cline{2-5}
& \ding{51} & sparse & specific mean & 2135461 \\ \cline{2-5}
& \ding{51} & sparse & specific median & 2134783 \\ \hline
\multicolumn{4}{|c|}{standard} & 2136995 \\ \hline
\multicolumn{4}{|c|}{aeiou} & 2133488 \\ \hline
\multicolumn{4}{|c|}{Chapin hst diff} & 2135905 \\ \hline
\multicolumn{4}{|c|}{Chapin inv} & 2135217 \\ \hline
\multicolumn{4}{|c|}{Chapin inv log} & x \\ \hline
\end{tabular}

%-first column only, all columns, with feedback corrections, compare
%-chapin's metrics in my impl not quite as good as handpicked, same in his

book1, first col only:
\begin{itemize}
  \item first col only brings almost no benefit when using the handpicked order
  \item for first col only almost all badness variants perform better than
  handpicked or Chapin's
  \item with weighting is usually better
  \item when entropy code length prediction is turned on, it can affect the
  result in either direction, but usually not by much. possible explanation: its
  output are low integers instead of (higher) floats, this takes some precision
  away. when the prediction is correct for most values, it affects the result
  positively (as it's theoretically ideal), but if a couple of values are off by
  a bit, it introduces a considerable relative error (arithmetic coding?). also,
  they seem to be pretty useless on their own (without either mtf prediction or
  weighting).
  \item all kinds of mtf predicition have a positive influence, unless when
  combined with entropy length prediction, in which case results vary (gonna
  put this on entr len pred, see above). both generic and specific median give
  bad results when combined with entr len pred. this could be because the median
  tips the entr len predictor into incorrect results (some form of rounding
  error, see above) and so this could be reversed for a different file, but
  maybe not. generic prediction works much better when no weigthing occurs,
  otherwise the specific version is marginally better. no significant difference
  between mean and median, except for what's described before.
\end{itemize}

all cols:
\begin{itemize}
  \item when only considering first col reordering, almost all badness variants
  were better than handpicked and Chapin's, with all cols only a few. some are
  even worse than the standard ordering. so badness is good at what it was
  designed to do, but obviously not all variants make good reorderings for every
  column.
  \item when using all cols, much higher compression gain (up to 5x). this means
  reordering of subsequent cols to make 2nd- 3rd- etc order transitions better
  is very relevant compared to just considering the first column. this makes
  sense: compression is gained where transitions are reordered. if there are n
  transitions of the first order, there are up to \(n^2\) of the second, \(n^3\)
  of the third etc. but at a certain point the k-th order context only contains
  one or a few symbols, making reorderings not seem useful anymore. if the file
  is larger and contains more contexts, reordering further into the columns can
  become useful. (it becomes useless at the latest when all the columns in the
  bw table up to that column are unique)
  %TODO more?
\end{itemize}

%TODO update in case parameters to LKH change
%TODO more files, book2, some non-text

\section{Further Work}

%-sorting more useful if using a different list update algo? (with slower
%	convergence)
%-analyze longer transitions, i.e. not all a->b, a!=b, but all a->b->c, a!=b!=c
%	etc. reduces necessity for predictions, extreme case: try all possible
%	orders
%-bzip does bw block wise. this means more transitions. reordering more useful
%	there? maybe that's why chapin's results were better

\chapter{Ordering more columns independently}

%-transition from last 2nd-order context in first 1st-order context to first
% 2nd-order context in second 1st-order context relevant?

\chapter{Exceptions to MTF}

\section{Previous Work}

chapin tried using two list update algos
\cite{chapin2000switching,chapin2001diss}. also assumes that one algo isn't
universally the best

wirth, moffat, altered or no list update \cite{wirth2001ranks}

\bibliographystyle{plain}
\bibliography{bib}
\end{document}
