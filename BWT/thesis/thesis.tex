\documentclass[a4paper]{scrreprt}

\begin{document}

\chapter{Intro}

%-def compression (decompression necessary)
%-explain bw-mtf-entropy, mtf can be skipped
%-explain simple version of bw used here, no rle
%-definitions: context block, symbol (byte in this thesis), natural sort order

\chapter{Simple Sorting}

%-overwork in context transitions
%-metrics to rank transitions, tsp to find a tour
%-problem finding atsp heuristic, only one i found only supports ints, give 
%	error bounds for the scaling
%-chapins metrics work on the bw code
%-badness works on the mtf of the bw, with modifications attempts to give the
%	number of bits a transition costs:
%	-make a list of the best possible alphabet the left side of a transition
%		can leave for the right side
%	-compare those with the actual codes you get when you mtf encode the left
%		and right side together
%	-the sum of all the differences is the badness
%	-variants:
%		-weighting: divide the badness by the number of new symbols on the right
%			side, so transitions to a larger context don't get a massive
%			penalty
%		-new penalty: new symbols in the right side that didn't appear in the
%			left side need to receive a penalty instead of assuming the minimum
%			possible (mean mtf code for mtf codes of at least the min possible,
%			symbol specific)
%			prediction for the block that ends up as the first will be way off
%		-entropy code len: don't take the plain difference but the difference
%			between the number of bits the entropy coder will use. this can
%			give you the exact number of bits this transition costs
%			approximation with const values for zeros: zeros will all get the
%			same (longest possible) code length, other rare symbols may be
%			affected
%			approximation with curve fitting: zeros will get about the same
%			code lengths as the surrounding symbols, but all of the rarer
%			symbols (>10?) will have (significantly) longer codes as there are
%			more symbols to encode. higher value mtf codes are usually sparse,
%			but can't be sure how many zeros will remain after reordering
%			curve fitting sparse: assume there will be the same amount of
%			zeros after reordering. give zeros the same length as their
%			neighbors, even though there can't be a huffman code with those
%			code lengths and that many symbols 
%	-describe things needed for an ideal badness metric (new penalty,entropy
%		len, weighting?), then give approximations
%	-another point of non-optimality: which transition to choose as a starting
%		point? need to break up the baddest transition
%	-need need a new penalty dict that is specific for the symbol being encoded
%		i.e. what is the average code for e.g. 'a' given that the minimum
%		possible is n
%	-new penalty predictor that gives the average code for a specific symbol
% 		that is new in the block, with a given mimimum
%	-much higher compression benefit when sorting all columns instead of just
%		the first one -> a lot of the compression benefit comes from the higher
% 		order transitions
%	-after comparing predictions with actual values, do another pass of
%		calculating transitions, but pass a compensation summand that will be
%		added to every prediction, so that the average is 0

The reason Burrows-Wheeler based compression algorithms are so effective is that
the Burrows-Wheeler Transform (BWT) creates sequences of symbols that appear in
the same context. If the input file is suitable for context-based
compression, these sequences will consist of only a few distinct symbols with
lots of repetition, leading to many small numbers in the MTF code.

But any time the context changes, in the Burrows-Wheeler code a new sequence
begins that is in general not related to the previous one. This incurs some
overwork\cite{bitner1979heuristics} for the MTF coder as it has to adapt to the
new context, resulting in larger codes for symbols that were not requested in
the previous sequence.

The idea behind using different sort orders is to order the contexts in a way
that reduces the overwork caused by a transition from one context to the next,
resulting in lower MTF codes and higher compression.

In order to find a suitable reordering, you need to do the BWT on the input data
to get the context blocks, rank all possible transitions between blocks with a
cost metric and find a sort order based on the costs. Finding the ordering with
given costs is an instance of the traveling salesman problem, where the nodes
are the context blocks and the distances between them are the cost of the
transition.

\section{Presious Work}

chapin original \cite{chapin1998sort,chapin2001diss}
merge with next section unless there's more

\section{Chapin's Metrics}

aeiou

Chapin uses four different cost metrics, three of which I have reimplemented for
comparison with my own metric.\footnote{The fourth uses the Kullback-Leibler
distance, but is unclear about how to deal with zeros in the PMF.} All of them
analyze the histograms of symbol appearances for every context block and try to
give a measure of how similar they are. That means: For each context block
(generated by BW encoding the input with the natural sort order), make a
histogram of the number of symbol appearances in that block, i.e.
a mapping from each possible symbol to the number of times it appears in that
block.

In Chapin's first metric, two histograms are compared by taking the
logarithm\footnote{Chapin doesn't specify to which base. In my implementation,
I use the natural logarithm.} of all of the symbol counts, calculating, for each
symbol, the differences between the logarithms in both histograms and summing up
the squares of all of the differences. The result is used as a measure of
similarity between the two contexts.

For the second metric, the symbol counts from the histograms are written to a
list in decreasing order. The cost of a transition is the number of
inversions\cite{sleator1985amortized} between the two corresponding lists.

The third metric is just the logarithm of the second metric.\footnote{Again, no
base was specified, I use the natural logarithm.}

-not comparable with my data because of variations in tsp and the actual
compression algo

\section{The Badness Metric}

My own metric attempts to give a value denoting how bad any given transition is
and is hence called the badness metric.\footnote{Only after I had given it that
name and used it all over my code did it come to my mind that assigning a
badness value is really the purpose of any such metric.} Other than Chapin's
metrics, it doesn't operate on the BW code, but on the MTF code it generates
and so, in a sense, is ``closer to the compression''.


\subsection{Ugly LKH}

The badness values generated are not necessarily symmetric, i.e. the badness of
the transition from context x to y is not necessarily the badness of y to x.
This has the unfortunate consequence that I now need a heuristic solver for the
asymmetrical traveling salesman problem, which is much harder to find than a
heuristic for the ``ordinary'' symmetrical TSP. I finally found
LKH\cite{helsgaun2000lkh}\footnote{http://www.akira.ruc.dk/~keld/research/LKH/},
which can approximate solutions to the asymmetrical TSP. Unfortunately, it can
not handle floating point input, so I have to do some ugly scaling.

%-describe and give error bounds

\subsection{Basic Operation}

To explain the badness metric, I need to introduce the partial MTF code that
will be used on appropriate parts of the BW code. This is not absolutely
necessary to get the actual badness value, but useful to understand why the
metric does what it does.

The partial MTF differs from the regular MTF code only in that you start with an
empty alphabet and encode an escape code (I will use -1) anytime a symbol
appears that isn't already in the alphabet.
E.g., the ASCII encoded string ``aabcab'' encoded (byte-wise) with regular MTF
would yield \([97, 0, 98, 99, 2, 2]\), encoded with partial MTF it would be
\([-1, 0, -1, -1, 2, 2]\).

Of course this means that it can't be decoded anymore, but we only need it to
illustrate the operation of the metric.

To compute the badness value of a transition, the metric needs the context
blocks for both sides of the transition, i.e. the BW code belonging to the
symbol that's being transitioned from or to, respectively.

It then produces the partial MTF code of the destination block (the right
side), and of the concatenation of the source and destination block (combined
partial MTF).
So it  basically pretends that the two blocks were sorted one after the other,
and then attempts to give an indication of how bad this would be for the
compression.

Looking at the partial MTF code of the destination side of the transition, the
metric assumes that all recurring codes (i.e. those that aren't escape codes)
are not interesting for the purposes of ranking transitions, as they would be
the same no matter what comes before or after.\footnote{This assumtion is not
actually correct, as I will explain in the next section.}

What is of interest, are the escape codes on the right side of the transition,
because this is where what has previously been encoded can influence the MTF
codes in the final result. When the input file is finally encoded with the
(full) MTF, each of the escape symbols will be replaced by an actual code. How
high those codes are, determines the compression lost by the transition between
contexts.

The metric compiles the ideal MTF alphabet the left side can ``leave'' for the
right side.

E.g., the first code in the partial MTF of the right side is necessarily an
escape code. It would be ideal, if the last symbol of the left side were the
first symbol of the right side. That would mean that it is at the front of the
alphabet when the transition occurrs, and so that first escape code would, in
the full MTF be encoded as a 0 (and smaller codes are better). The ideal full
MTF code for the next escape code is 1 (since the code 0 in the alphabet has
already been taken by whatever symbol came before it) etc.
%example? pseudo code?

The metric can then compare the ideal codes with the actual codes in the
combined partial MTF at the positions where the escape symbols are in the
partial MTF of just the right side. The differences between actual code and
ideal code are summed up to form the badness of the transition.

What can also happen is that the part of the combined partial MTF belonging to
the right side still contains escape codes. This means that the symbols encoded
by them do not appear in the left side of the transition at all. In this case,
the metric assumes the best possible code for that symbol, which is equal to the
number of escape symbols in the combined partial MTF up to that point (i.e. the
metric assumes that whatever context block ends up preceding the left side
of this transition will leave an ideal alphabet).

\subsection{Ideality, under some circumstances}

The metric assumes that the partial MTF codes of the right side of the
transition that are not escape codes don't change no matter the order that is
finally selected, and thus aren't interesting. This assumption is incorrect
since, when a different reordering is selected, the BW code of the context
blocks is also reordered, and so the generated MTF code will (probably) be
different. For Chapin's metrics, this doesn't matter because they only care
which symbol occurrs how often, not in which order.

So we'll consider a slightly different problem than finding an ideal global
reordering: instead of looking for a sort order by which the entire BW table
will be sorted, we look for an order by which only the first column of the BW
table will be sorted, all other columns will use the natural order (which was
used to generate the context blocks that are fed to the metric).

This is equivalent to finding a reordering of the fixed context blocks the
metric deals with. This means that recurring symbols in the partial MTF never
change, no matter what order is chosen for the first column, and so the
assumption the metric makes is true.

Let's further assume that we have two oracles that know which reordering will
finally be selected. When the metric encounters an escape symbol in the right
side of the combined partial MTF (i.e. a symbol that appears in the right side
doesn't appear in the left side), it has to assume the best possible code for
lack of information. The first oracle can provide the actual codes that will be
at those positions in the final computed order.

The second oracle can provide the codeword lengths in bits of the entropy coder
for every MTF code appearing in the final reordered and MTF encoded file.

If the badness metric is modified to use the predictions of the MTF code oracle
and sums up the differences between the entropy codeword lengths of the actual
and ideal MTF codes instead of the differences between the codes themselves, it
is the ideal metric for generating a reordering for only the first column.

Since only the first column gets reordered, recurring symbols within a
context block are fixed and have no influence, good or bad, on the compression
rate. The only thing that can make a difference are the MTF codes of symbols
occurring for the first time in the block. The badness metric records the
difference between the number of bits that are used if the left side of the
transition leaves an ideal alphabet for the right side, and the number of bits
that are actually used.

That way, if the transition (a,b) has a badness of n, and the transition (a,c)
has a badness of n+m, all other transition's badness values being equal, if the
transition (a,c) is used, the result will be m bits larger than if the
transition (a,b) was used.

If the badness values of all possible transitions are provided as input to an
exact TSP solver, it will generate the best possible reordering for only the
first column of the BW table.

\subsection{Variants}

There are three variants of the badness metric, two of which are attempting to
approximate the predictions of the oracles from the previous section.

\subsubsection{Weighting by number of symbols}

The computed badness value is divided by the number of distinct symbols in the
right side of the transition (i.e. the number of escape codes in the partial
MTF). This is supposed to counteract the effect where transitions whose right
side has many differenct symbols get a much worse rating than if they have less,
because every escape code leads to some badness being added to the total value.
The resulting value can be characterized as badness per distinct symbol.

\subsubsection{Predicting code lengths of the entropy coder}

%TODO: arithmetic coding? otherwise just call it Huffman coding
This modification tries to approximate the behavior of the oracle predicting the
code lengths of the entropy coder.

When adding to the badness value, instead of adding the difference between the
actual and the ideal MTF codes, the difference between a prediction of the
entropy code lengths of the actual and ideal MTF codes is added.

Getting pretty good predictions for a static entropy coder is relatively easy:
since different reorderings only has a little effect on the produced MTF code,
the entropy code lengths for any reordering should be within a small margin of
error of each other. In the case of static Huffman coding, many of the codes for
low MTF codes may not change at all.

So in order to get the predictions, the input file is simply BW- and MTF encoded
with the natural sort order and the lengths of the codewords are recorded that
the entropy coder would use for each MTF code.

There is a problem with this simple method when not all possible MTF codes
appear in the code that is used to make the predictions. This is usually the
case with e.g. ASCII text files, which tend to use less than 100 distinct
symbols\footnote{book1 from the calgary corpus uses 82 different characters:
upper- and lowercase alphabet, 10 digits and some punctuation and control
characters.}, so MTF codes above that number only occur once when that symbol is
fetched to the front of the MTF alphabet, and may not occur at all.
When a different reordering is chosen, a different MTF alphabet will probably be
in place at the time such a symbol is requested, and the resulting MTF code may
be shifted a little. That means that an MTF code for which a prediction exists
may not be requested at all, but a code that is requested has no prediction.

To solve this, I have developed two strategies, ``complete'' and ``sparse''
predictions.
The ``complete'' method modifies the symbol frequencies of the MTF code that are
used as weights by the entropy coder. Every MTF code that doesn't appear but is
smaller than the maximum code that does gets the weight \(\frac{1}{256}\). In
the case of Huffman coding this means that all these codes get a longer or
equally long entropy code than the codes that actually appear.

All other MTF codes that don't appear, but are greater than the maximum code
that does get weight \(\frac{1}{256^2}\), so their entropy codes are guaranteed
to be longer or equal to the ones with weight \(\frac{1}{256}\). The reasoning
behind this is that these codes are unlikely to appear in any reordering, e.g.
when encoding an ASCII text file, no codes above 127 will appear.

The ``sparse'' method assumes that no matter the reordering, there will always
be (high) MTF codes that don't appear (the histogram of MTF codes will be
sparse for high values).
It lets the entropy coder compute the codeword lengths with unaltered weights,
but inserts entropy code lengths for the missing MTF codes manually afterwards.
Each missing MTF code is assigned the same code length as the next smaller MTF
code that does appear. The reasoning is that, when MTF codes appear for one
reordering that didn't for another, they're just shifted around from codes that
don't appear anymore. Of course this means that there isn't actually an entropy
code with the code lengths that were predicted (since the predictor has
manually introduced collisions).

\subsubsection{Predicting MTF codes for new symbols}

This modification aims at approximating the oracle that can predict MTF codes of
escape codes in the right side of the combined partial MTF of a transition. When
the metric encounters an escape code, instead of assuming that whatever
context came before left an ideal alphabet, it can use the prediction, which is
hopefully more accurate.

Making these predictions is more complicated than those for the entropy code
lengths. I have again developed two strategies, ``generic'' and ``specific'',
which assume that the distribution of MTF codes will be similar no matter the
order.

The generic predictor does a BW encode on the input file with the natural order,
then encodes this with MTF. It then records, for every possible MTF code, the
average value of all MTF codes that are greater or equal to it.

The specific predictor also takes into account the underlying BW code. So it
records, for every possible MTF code and every possible symbol, the average
value of the MTF codes that are greater or equal to it and that encode the
underlying symbol.

The averaging function for both predictors can be the arithmetic mean or the
median.

\subsection{Evaluating the performance of the predictors}

%-for only first col and for all cols

\subsubsection{Feeding back correction values}

\subsection{Results}

%-first column only, all columns, with feedback corrections, compare

TODO update in case parameters to LKH change

\section{Further Work}

%-sorting more useful if using a different list update algo? (with slower
%	convergence)
%-analyze longer transitions, i.e. not all a->b, a!=b, but all a->b->c, a!=b!=c
%	etc. reduces necessity for predictions, extreme case: try all possible
%	orders

\chapter{Exceptions to MTF}

\section{Previous Work}

chapin tried using two list update algos
\cite{chapin2000switching,chapin2001diss}. also assumes that one algo isn't
universally the best

wirth, moffat, altered or no list update \cite{wirth2001ranks}

\bibliographystyle{plain}
\bibliography{bib}
\end{document}
