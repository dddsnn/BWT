import bwt.huffman as hf
import bwt.coder as cd
import sys
from bwt import *
import math
import numpy as np

def select_mtf_exceptions(bw, min_length, threshold):
    """Select symbols whose context blocks should be excluded from the MTF
    phase.
    """
    mtf = cd.mtf_encode(bw.encoded)
    contexts = {s:context_block(mtf, bw.firsts, bytes([s]))
                for s in set(bw.encoded)}
    context_lengths = {s:len(contexts[s]) for s in contexts}
    context_means = {s:np.mean(contexts[s]) for s in contexts}
    result = [bytes([s]) for s in contexts
              if context_lengths[s] >= min_length
              and context_means[s] >= threshold]
    return result

def compare_new_penalty_predictions(aux_data, orders, new_penalty_log):
    """Compare the predictions for new codes with actual values.

    Args:
        aux_data: The bwt.AuxData for the input that the transitions were
            computed for.
        orders: A list of orders that is the result of the TSP on the computed
            transitions. Same format as in coder.bw_encode().
        new_penalty_log: The log dictionary generated by the badness metric when
            provided with the appropriate option.

    Returns:
        A list of tuples. For every transition between context blocks according
            to the given orders, for each new symbol on the right side of the
            transition a tuple is created containing the context of the block,
            the MTF code of the first appearance of the symbol in the actual
            code, and the logged prediction for that symbol.
            Symbols in the first block can't be compared, because the first
            block is not the right side of any transition.
    """
    bw_code = cd.bw_encode(aux_data.raw, orders)
    mtf = cd.mtf_encode(bw_code.encoded)
    bw_mtf = list(zip(bw_code.encoded, mtf))
    result = []
    # can only handle the first order for the moment
    order = orders[0]
    transitions = zip(order[:-1], order[1:])
    for trs in transitions:
        if trs not in new_penalty_log:
            # there were not predictions necessary for this transition, skip
            continue
        # get the block of mtf-bw tuples corresponding to the right side of the
        # transition
        block = context_block(bw_mtf, bw_code.firsts, trs[1])
        predictions = new_penalty_log[trs]
        # for every prediction, record the current sequence, the actual code
        # and the prediction
        for s in sorted(predictions.keys()):
            # actual code is the mtf code of the first appearance of the symbol
            actual = next(b[1] for b in block if b[0] == s)
            result.append((trs[1], actual, predictions[s]))
    return result

def compare_entropy_len_predictions(aux_data, orders, predictions):
    """Compare the predictions for entropy code lengths with actual values.

    Args:
        aux_data: The bwt.AuxData for the input.
        orders: The computed sort orders yielding the actual entropy code
            lengths.
        predictions: A dictionary mapping MTF codes to the predicted entropy
            code length, as returned by bwt.analyzer.huffman_codeword_lengths().

    Returns:
        A list of tuples. Each tuple contains an MTF code, its actual entropy
        codeword length when the file is BW encoded with the given orders, and
        the predicted length for that MTF code.
    """
    mtf_code = cd.mtf_encode(cd.bw_encode(aux_data.raw, orders).encoded)
    freqs = hf.symbol_frequencies(mtf_code)
    actual = hf.codeword_lengths(freqs)
    result = []
    for s in actual:
        result.append((s, actual[s], predictions[s]))
    return result

def context_block(code, firsts, seq):
    """Get a specific BW block.

    Gets the block of BW code corresponding to a specific sequence at the
    beginning of the line.

    Args:
        code: An iterable from which the result will be selected.
        firsts: The sequences at the beginning of the BW table. Must have the
            same length as code and correspond to it. At least one element must
            contain seq as a substring.
        seq: The sequence at the beginning of the line whose block should be
            returned.

    Returns:
        The block of BW code whose first lines begin with seq.
    """
    # first index matching seq
    left = next(i for i, s in enumerate(firsts) if s[:len(seq)] == seq)
    right = left + 1
    while right < len(firsts) and firsts[right][:len(seq)] == seq:
        right += 1
    context_block = code[left:right]
    return context_block

def analyze_partial_mtf(mtf_code):
    """Get various stats about a partial MTF mtf_code.

    Args:
        mtf_code: The partial MTF to be analyzed.

    Returns:
        A bwt.PartialMTFAnalysisResult of the mtf_code.
    """
    raw = mtf_code
    length = len(mtf_code)
    sorted_code = sorted(mtf_code)
    max_code = sorted_code[-1]
    # remove -1s
    sorted_code = [c for c in sorted_code if c != -1]
    # number of all recurring symbols (those that aren't -1)
    l = len(sorted_code)
    # number of -1s is number of all symbols minus number of all except -1s
    num_chars = length - l
    # number of recurring characters (not -1)
    length_rec = length - num_chars
    result = PartialMTFAnalysisResult(raw, length, length_rec, num_chars,
                                      max_code)
    return result

def num_inversions(list_a, list_b):
    """Compute the number of inversions between two lists.

    The number of inversions is the number of times any pair of elements occur
    in a different order in the two lists.
    The lists need to be permutations of each other.

    Args:
        list_a: The first list.
        list_b: The second list.

    Returns:
        The number of inverions between list_a and list_b.
    """
    inv = 0
    for i_a, x in enumerate(list_a):
        i_b = list_b.index(x)
        # all the items coming before x in list_b
        before_list = list_b[:i_b]
        # all the items coming after x in hst_a
        after_list = list_a[i_a + 1:]
        for a in after_list:
            if a in before_list:
                # inverions found, increment inv
                inv += 1
    return inv

def make_histogram(bs):
    """Make a histogram of symbol appearances for a bytes object.

    Args:
        bs: The input bytes object.

    Returns:
        A histogram giving count of appearance in bs for every byte value.
        E.g.: bytes([1, 1, 2, 3]) -> {0: 0, 1: 2, 3: 1, 4: 0, 5: 0 ...}
    """
    # initialize 0 for all possible bytes
    histogram = {i:0 for i in range(256)}
    for b in bs:
        histogram[b] += 1
    return histogram

def huffman_codeword_lengths(mtf_code, zero_compensation):
    """Estimate huffman code word lengths for data similar to the input.

    For each byte value, give an estimate of the length in bit of the huffman
    code word if the bytes_ were encoded with huffman while also including
    values that usually wouldn't be encoded because they don't occur. Use for
    MTF codes.
    """
    freqs = hf.symbol_frequencies(mtf_code)
    if zero_compensation == 'complete':
        # find the highest value symbol that appears in the mtf code
        max_nonzero = max(freqs)
        # give all mtf codes that don't appear in mtf_code and are lower than
        # max_nonzero weight 1/256, so they get a longer code than any of the
        # ones actually appearing
        for i in range(max_nonzero):
            if i not in freqs:
                freqs[i] = 1 / 256
        # give all mtf codes that don't appear and are higher than max_nonzero
        # weight 1/256**2 so they get even longer codes
        for i in range(max_nonzero + 1, 256):
            if i not in freqs:
                freqs[i] = 1 / (256 ** 2)
        return hf.codeword_lengths(freqs)
    elif zero_compensation == 'sparse':
        hf_len = hf.codeword_lengths(freqs)
        # go through the dict and give each non-occurring symbol the same
        # codeword length as its predecessor
        last_len = hf_len[min(hf_len)]
        for i in range(256):
            if i not in hf_len:
                hf_len[i] = last_len
            else:
                last_len = hf_len[i]
        return hf_len
    else:
        raise ValueError('{0} is not a valid value for zero_compensation. Must'
                         ' be one of False, \'complete\' or \'sparse\'.'
                         .format(zero_compensation))

def mtf_avg_steps(bw_code, mtf_code, avg_func):
    """Make a dictionary of average MTF codes of a minimum value.

    Args:
        bw_code: BW code as a bytes object.
        mtf_code: The MTF code of bw_code, a list of integers between 0 and 255.
        avg_func: The function to be used for averaging the list, e.g. mean or
            median.

    Returns:
        A dictionary containing, for every integer n between 0 and 255, the
        average of all values in mtf_code that are greater or equal than n,
        averaged by the given function.
        The dictionary also maps tuples (sym, n) to the average of all values
        in mtf_code that encode the symbol sym from bw_code and are greater or
        equal than n.
    """
    def mean_steps_generic(mtf_code):
        result = {}
        for n in range(256):
            l = [x for x in mtf_code if x >= n]
            if l:
                result[n] = avg_func(l)
            else:
                result[n] = n
        return result
    # first the generic means, not taking into account the underlying symbol
    result = mean_steps_generic(mtf_code)
    # now for every underlying symbol, for every mtf code
    symbol_mtf_dict = {s:[] for s in set(bw_code)}
    for sym, mtf in zip(bw_code, mtf_code):
        symbol_mtf_dict[sym].append(mtf)
    # make the generic means dict for every symbol's list of mtf codes
    for sym in symbol_mtf_dict:
        for n, mean_dev in mean_steps_generic(symbol_mtf_dict[sym]).items():
            result[(sym, n)] = mean_dev
    return result

def analyze_transitions(aux_data, metric, prefix=b''):
    """Analyze all possible transitions between values of a byte string.

    Args:
        aux_data: The bwt.AuxData object for the input.
        metric: A tuple of metric name and a dictionary with metric options
        passed to the metric function as keyword arguments. The string 'metric_'
        will be prepended to this name to get the actual function.

    Returns:
        A dictionary mapping each pair of byte values from bs to that
        transition's weight according to the given metric.
    """
    metric_name = metric[0]
    metric_opts = metric[1]
    an_func = getattr(sys.modules[__name__], 'metric_' + metric_name)
    firsts = aux_data.firsts
    transitions = {(a, b): an_func(a, b, aux_data, **metric_opts)
                   for a in firsts
                   for b in firsts
                   if a != b and a[:len(prefix)] == prefix
                   and b[:len(prefix)] == prefix and len(a) == len(prefix) + 1
                   and len(b) == len(prefix) + 1}
    return transitions

def metric_chapin_hst_diff(first_seq_a, first_seq_b, aux_data, **kwargs):
    hst_a = aux_data.bw_subhistograms[first_seq_a]
    hst_b = aux_data.bw_subhistograms[first_seq_b]

    # CHAPIN: sum of squares of differences of logs
    log_diffs = []
    for k in hst_a:
        # get the logs
        # TODO not sure replacing -inf with 0 doesn't affect result
        if hst_a[k] == 0:
            log_a = 0
        else:
            log_a = math.log(hst_a[k])
        if hst_b[k] == 0:
            log_b = 0
        else:
            log_b = math.log(hst_b[k])
        log_diffs.append(log_a - log_b)
    # squares of differences of logarithms in the histograms
    metric = sum([d ** 2 for d in log_diffs])
    return metric

def metric_chapin_kl(first_seq_a, first_seq_b, aux_data, **kwargs):
    hst_a = aux_data.bw_subhistograms[first_seq_a]
    hst_b = aux_data.bw_subhistograms[first_seq_b]

    # CHAPIN: kullback-leibler
    logterms = []
    for k in hst_a:
        if hst_a[k] == 0 or hst_b[k] == 0:
            # in case of zeros, ignore this term
            # TODO http://mathoverflow.net/questions/72668/how-to-compute-kl-divergence-when-pmf-contains-0s
            continue
        logterms.append(math.log(hst_b[k] / hst_a[k]) * hst_b[k])
    metric = sum(logterms)
    return metric

def metric_chapin_inv(first_seq_a, first_seq_b, aux_data, **kwargs):
    freq_list_a = aux_data.freq_lists[first_seq_a]
    freq_list_b = aux_data.freq_lists[first_seq_b]

    # get options from kwargs
    if 'log' in kwargs:
        log = kwargs['log']
    else:
        log = False

    # CHAPIN: number of inversion between ordered histograms
    # metric is the number of inversions between the two lists
    metric = num_inversions(freq_list_a, freq_list_b)
    if log:
        # calculate log, if requested in the options
        if metric == 0:
            return metric
        else:
            return math.log(metric)
    else:
        return metric

def metric_badness(first_seq_a, first_seq_b, aux_data, **kwargs):
    """Assign a "badness" value to a given transition.

    Compares the ordering of the MTF alphabet the left side of the transition
    "leaves" for the right side with the ideal ordering that would yield the
    greatest compression benefit, and gives a value for non-ideal orderings
    denoting how "bad" the transition is.

    Args:
        first_seq_a: The sequence of bytes whose context block is the left side
            of the transition, as a bytes object.
        first_seq_b: Same as first_seq_a, for the right side of the transition.
        aux_data: The bwt.AuxData for the input file.
        kwargs: Keyword Arguments:
            weighted: Boolean, whether the badness should be weighted with the
                number of new symbols in the right side. Defaults to False.
            new_penalty: Boolean, whether a penalty should be given for new
                symbols in the right side that don't appear in the left side.
                Defaults to False.
            entropy_code_len: Boolean, whether the difference in the number of
                bits used by the entropy coder should be used to compute the
                badness, rather than the plain difference between the codes.
                Defaults to False.
            new_penalty_log: A dictionary to which the predictions of mtf codes
                that aren't know are written. If it doesn't exist, a new key
                (first_seq_a, first_seq_b) is created and a dictionary is
                stored, mapping the underlying symbol of the BW code for which
                the MTF code is predicted to the prediction.

    Returns:
        A number denoting how "bad" the given transition is.
    """
    pt_mtf_b = aux_data.partial_mtf_subcodes[first_seq_b]
    pt_mtf_ab = aux_data.partial_mtf_subcodes[(first_seq_a, first_seq_b)]
    an_a = aux_data.partial_mtf_analyses[first_seq_a]
    an_b = aux_data.partial_mtf_analyses[first_seq_b]
    mtf_steps = aux_data.mtf_mean_steps

    # get options from kwargs
    if 'weighted' in kwargs:
        weighted = kwargs['weighted']
    else:
        weighted = False
    if 'new_penalty' in kwargs:
        new_penalty = kwargs['new_penalty']
        if new_penalty not in [False, 'generic_mean', 'generic_median',
                               'specific_mean', 'specific_median']:
            raise ValueError('{0} is not a valid value for new_penalty. Must be'
                             ' one of False, \'generic_mean\', \'generic_median'
                             '\', \'specific_mean\' or \'specific_median\'.'
                             .format(new_penalty))
        if new_penalty in ['specific_mean', 'specific_median']:
            bw_subcode_b = aux_data.bw_subcodes[first_seq_b]
        if new_penalty in ['generic_mean', 'specific_mean']:
            mtf_steps = aux_data.mtf_mean_steps
        if new_penalty in ['generic_median', 'specific_median']:
            mtf_steps = aux_data.mtf_median_steps
    else:
        new_penalty = False
    if 'entropy_code_len' in kwargs:
        entropy_code_len = kwargs['entropy_code_len']
        if entropy_code_len not in [False, 'complete', 'sparse']:
            raise ValueError('{0} is not a valid value for entropy_code_len. '
                             'Must be one of False, \'complete\' or \'sparse\'.'
                             .format(kwargs['entropy_code_len']))
        if kwargs['entropy_code_len'] == 'complete':
            hf_len = aux_data.huffman_codeword_lengths_complete
        elif kwargs['entropy_code_len'] == 'sparse':
            hf_len = aux_data.huffman_codeword_lengths_sparse
    else:
        entropy_code_len = False
    if 'new_penalty_log' in kwargs:
        new_penalty_log = kwargs['new_penalty_log']
        bw_subcode_b = aux_data.bw_subcodes[first_seq_b]
    else:
        new_penalty_log = False
    if 'mtf_prediction_correction' in kwargs:
        mtf_prediction_correction = kwargs['mtf_prediction_correction']
    else:
        mtf_prediction_correction = 0

    badness = 0
    # make list of tuples (index in partial mtf of right side, optimal code) for
    # every new symbol in the partial mtf of the right side
    ideal_codes = []
    min_possible = 0
    for i, s in enumerate(pt_mtf_b):
        if s != -1:
            # recurring symbol, ignore
            continue
        ideal_codes.append((i, min_possible))
        min_possible += 1
    # length of the mtf code of the left side
    length_left = len(pt_mtf_ab) - len(pt_mtf_b)
    # the minimal possible code for any new symbols in the right side of the
    # combined code
    min_possible = an_a.num_chars
    # now compare the ideal codes with the actual ones and add the difference
    # to the badness
    for i, ideal in ideal_codes:
        actual = pt_mtf_ab[i + length_left]
        if actual == -1:
            # new symbol in the combined mtf
            if new_penalty in ['generic_mean', 'generic_median']:
                # give a generic penalty for a new symbol, if this was requested
                # in the options
                assumed_actual = mtf_steps[min_possible]
                assumed_actual += mtf_prediction_correction
            elif new_penalty in ['specific_mean', 'specific_median']:
                # give a penalty specific to the underlying symbol from the
                # bw code
                assumed_actual = mtf_steps[(bw_subcode_b[i], min_possible)]
                assumed_actual += mtf_prediction_correction
            else:
                # otherwise, assume the best possible
                assumed_actual = min_possible
                assumed_actual += mtf_prediction_correction
            # write to the new penalty log if it exists
            if new_penalty_log != False:
                if (first_seq_a, first_seq_b) not in new_penalty_log:
                    new_penalty_log[(first_seq_a, first_seq_b)] = {}
                new_penalty_log[(first_seq_a,
                                 first_seq_b)][bw_subcode_b[i]] = assumed_actual
            if entropy_code_len:
                # add the approximation of the actual number of bits this
                # transition will cost in the entropy coder, if requested in
                # the options
                # assumed actual isn't necessarily an integer. round to the next
                # int just in case, so we still find it in the dict
                assumed_actual = int(round(assumed_actual))
                if assumed_actual > 255:
                    # in case it gets rounded out of the range of valid
                    # dict keys
                    assumed_actual = 255
                huff_dist = hf_len[assumed_actual] - hf_len[ideal]
                if huff_dist < 0:
                    # for high code values, higher codes can accidentally be
                    # smaller than lower ones. set to 0 if this happens
                    huff_dist = 0
                badness += huff_dist
            else:
                # otherwise just add the distance
                badness += assumed_actual - ideal
            # minimal possible code for new symbols is now increased
            min_possible += 1
        else:
            if entropy_code_len:
                huff_dist = hf_len[actual] - hf_len[ideal]
                if huff_dist < 0:
                    huff_dist = 0
                badness += huff_dist
            else:
                badness += actual - ideal
    if weighted:
        # weight the result by the number of symbols in the right side, if this
        # was requested in the options
        badness /= an_b.num_chars
    return badness
